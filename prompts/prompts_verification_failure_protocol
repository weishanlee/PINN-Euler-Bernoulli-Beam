# Verification Failure Protocol

## ðŸš¨ Automated Failure Handling Procedures

This document defines the MANDATORY procedures for handling verification failures automatically without user intervention. The system must continue processing and log all decisions for final review in introduction.pdf.

## ðŸ“‹ Failure Types and Automated Responses

### 1. Title Mismatch (Similarity < 80%)

**Detection:**
```python
if title_similarity < 0.8:
    failure_type = "TITLE_MISMATCH"
```

**Automated Response:**
1. Delete the downloaded PDF immediately
2. Log failure with details
3. Try alternative search strategies
4. If no alternatives work, exclude citation
5. Continue with next paper

**Implementation:**
```python
def handle_title_mismatch(citation_key, expected_title, found_title, similarity):
    """Automatically handle title mismatch"""
    
    # Log the failure
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "citation_key": citation_key,
        "failure_type": "TITLE_MISMATCH",
        "expected_title": expected_title,
        "found_title": found_title,
        "similarity_score": similarity,
        "action": "EXCLUDED"
    }
    
    # Save to failure log
    with open('output/verification_failures.log', 'a') as f:
        f.write(json.dumps(log_entry) + '\n')
    
    # Try alternative search
    alternatives = [
        f'"{expected_title}"',  # Exact match search
        f'{expected_title} filetype:pdf',  # With PDF filter
        f'allintitle: {expected_title}',  # Title-only search
    ]
    
    for search_query in alternatives:
        alt_result = search_and_download(search_query)
        if alt_result and verify_title(alt_result, expected_title) >= 0.8:
            return True, "Found via alternative search"
    
    # All alternatives failed - exclude
    return False, "Excluded due to title mismatch"
```

### 2. Wrong ArXiv ID

**Detection:**
```python
if expected_arxiv_id and extracted_arxiv_id != expected_arxiv_id:
    failure_type = "ARXIV_MISMATCH"
```

**Automated Response:**
1. This is a critical failure - paper is definitely wrong
2. Delete PDF immediately
3. Search using the correct arXiv ID
4. If correct paper not found, exclude citation
5. Mark as high-priority issue in log

**Implementation:**
```python
def handle_arxiv_mismatch(citation_key, expected_id, found_id):
    """Handle arXiv ID mismatch - critical failure"""
    
    # Delete wrong paper
    wrong_pdf = f"output/papers/{citation_key}.pdf"
    if os.path.exists(wrong_pdf):
        os.remove(wrong_pdf)
    
    # Log critical failure
    with open('output/critical_failures.log', 'a') as f:
        f.write(f"CRITICAL: {citation_key} - Expected arXiv:{expected_id}, Found:{found_id}\n")
    
    # Try direct arXiv download
    correct_url = f"https://arxiv.org/pdf/{expected_id}.pdf"
    if download_from_url(correct_url, citation_key):
        return True, "Downloaded correct paper from arXiv"
    
    # Exclude if direct download fails
    return False, "Excluded - wrong arXiv ID and correct paper not found"
```

### 3. Author Name Mismatch

**Detection:**
```python
if not any(author_matches(exp_auth, found_auths) for exp_auth in expected_authors):
    failure_type = "AUTHOR_MISMATCH"
```

**Automated Response:**
1. This might be a different paper with similar title
2. Check if abstract contains expected keywords
3. If keywords match, keep but flag for review
4. If keywords don't match, exclude
5. Try searching with author + title combination

**Implementation:**
```python
def handle_author_mismatch(citation_key, expected_authors, found_authors, abstract):
    """Handle author mismatch with domain check"""
    
    # Check domain relevance as secondary verification
    domain_keywords = extract_domain_keywords(citation_key)
    relevance_score = calculate_domain_relevance(abstract, domain_keywords)
    
    if relevance_score > 0.5:
        # Probably right paper, wrong author format
        log_warning(f"{citation_key}: Author mismatch but domain matches - keeping")
        return True, "Kept despite author mismatch - domain verified"
    
    # Try author + title search
    for author in expected_authors:
        search_query = f'author:"{author}" "{expected_title}"'
        result = search_and_download(search_query)
        if result and verify_authors(result, expected_authors):
            return True, "Found via author search"
    
    # Exclude if all checks fail
    return False, "Excluded - author mismatch and domain irrelevant"
```

### 4. Domain/Field Mismatch

**Detection:**
```python
if domain_relevance_score < 0.3:
    failure_type = "DOMAIN_MISMATCH"
```

**Automated Response:**
1. Paper is from wrong field (e.g., chemistry paper for CS problem)
2. Delete immediately - no recovery possible
3. Log as wrong field
4. Exclude from bibliography
5. Add to domain mismatch statistics

**Implementation:**
```python
def handle_domain_mismatch(citation_key, abstract, expected_domain):
    """Handle papers from wrong field"""
    
    # Extract actual domain from abstract
    detected_domains = detect_paper_domain(abstract)
    
    # Log domain mismatch
    mismatch_entry = {
        "citation_key": citation_key,
        "expected_domain": expected_domain,
        "detected_domains": detected_domains,
        "action": "EXCLUDED_WRONG_FIELD"
    }
    
    with open('output/domain_mismatches.json', 'a') as f:
        f.write(json.dumps(mismatch_entry) + '\n')
    
    # Delete paper
    pdf_path = f"output/papers/{citation_key}.pdf"
    if os.path.exists(pdf_path):
        os.remove(pdf_path)
    
    return False, f"Excluded - wrong field: {detected_domains}"
```

### 5. Year Mismatch

**Detection:**
```python
if abs(expected_year - extracted_year) > 1:
    failure_type = "YEAR_MISMATCH"
```

**Automated Response:**
1. Might be preprint vs published version
2. Check if title and authors match
3. If both match, keep but note year difference
4. If other fields don't match, exclude
5. Try searching with specific year

**Implementation:**
```python
def handle_year_mismatch(citation_key, expected_year, found_year, title_match, author_match):
    """Handle publication year mismatch"""
    
    year_diff = abs(expected_year - found_year)
    
    if year_diff == 1 and title_match and author_match:
        # Likely preprint vs published version
        log_info(f"{citation_key}: Year mismatch by 1 year - keeping")
        return True, "Kept - likely preprint/published version difference"
    
    if year_diff > 2:
        # Too large difference
        return False, f"Excluded - year mismatch too large: {year_diff} years"
    
    # Try year-specific search
    search_query = f'"{expected_title}" year:{expected_year}'
    result = search_and_download(search_query)
    if result:
        return True, "Found correct year version"
    
    return False, "Excluded - year mismatch"
```

## ðŸ“Š Failure Statistics Tracking

### Required Statistics Files

```python
def initialize_failure_tracking():
    """Initialize all failure tracking files"""
    
    # Main failure statistics
    stats = {
        "total_attempts": 0,
        "successful": 0,
        "failed": 0,
        "failure_types": {
            "TITLE_MISMATCH": 0,
            "ARXIV_MISMATCH": 0,
            "AUTHOR_MISMATCH": 0,
            "DOMAIN_MISMATCH": 0,
            "YEAR_MISMATCH": 0,
            "DOWNLOAD_FAILED": 0
        },
        "recovery_success": 0,
        "excluded_citations": []
    }
    
    with open('output/verification_statistics.json', 'w') as f:
        json.dump(stats, f, indent=2)
```

### Update Statistics

```python
def update_failure_statistics(failure_type, citation_key, recovered=False):
    """Update failure statistics"""
    
    with open('output/verification_statistics.json', 'r') as f:
        stats = json.load(f)
    
    stats['failure_types'][failure_type] += 1
    
    if recovered:
        stats['recovery_success'] += 1
    else:
        stats['failed'] += 1
        stats['excluded_citations'].append(citation_key)
    
    with open('output/verification_statistics.json', 'w') as f:
        json.dump(stats, f, indent=2)
```

## ðŸ”„ Alternative Search Strategies

### Strategy Hierarchy

```python
SEARCH_STRATEGIES = [
    # Primary strategies
    ("exact_title", lambda t, a: f'"{t}"'),
    ("title_pdf", lambda t, a: f'{t} filetype:pdf'),
    ("arxiv_search", lambda t, a: f'site:arxiv.org {t}'),
    
    # Secondary strategies
    ("author_title", lambda t, a: f'author:"{a[0]}" {t}' if a else t),
    ("semantic_search", lambda t, a: semantic_paper_search(t)),
    ("google_scholar", lambda t, a: scholar_search(t, a)),
    
    # Last resort
    ("partial_title", lambda t, a: ' '.join(t.split()[:5])),
    ("keywords_only", lambda t, a: extract_keywords(t))
]

def try_all_search_strategies(expected_title, expected_authors):
    """Try all search strategies in order"""
    
    for strategy_name, strategy_func in SEARCH_STRATEGIES:
        try:
            search_query = strategy_func(expected_title, expected_authors)
            result = search_and_download(search_query)
            
            if result and verify_basic_match(result, expected_title):
                log_info(f"Success with strategy: {strategy_name}")
                return result
                
        except Exception as e:
            log_error(f"Strategy {strategy_name} failed: {e}")
    
    return None
```

## ðŸ“ Final Report Generation

### Include in Introduction.tex

```python
def generate_failure_report_for_introduction():
    """Generate failure report to include in introduction"""
    
    # Load all failure data
    with open('output/verification_statistics.json', 'r') as f:
        stats = json.load(f)
    
    report = f"""
\\subsection*{{Automated Reference Verification Report}}

During the automated verification process:
\\begin{{itemize}}
\\item {stats['total_attempts']} papers were attempted
\\item {stats['successful']} were successfully verified ({stats['successful']/stats['total_attempts']*100:.1f}\\%)
\\item {stats['failed']} failed verification and were excluded
\\item {stats['recovery_success']} were recovered using alternative searches
\\end{{itemize}}

Failure breakdown:
\\begin{{itemize}}
\\item Title mismatches: {stats['failure_types']['TITLE_MISMATCH']}
\\item Wrong arXiv IDs: {stats['failure_types']['ARXIV_MISMATCH']}
\\item Author mismatches: {stats['failure_types']['AUTHOR_MISMATCH']}
\\item Wrong field/domain: {stats['failure_types']['DOMAIN_MISMATCH']}
\\item Year mismatches: {stats['failure_types']['YEAR_MISMATCH']}
\\end{{itemize}}

All excluded citations have been logged in \\texttt{{verification\_failures.log}}.
The bibliography contains only verified, genuine papers relevant to the problem domain.
"""
    
    return report
```

## ðŸ›‘ Critical Rules

1. **NEVER** stop the process for user input
2. **ALWAYS** exclude papers that fail verification
3. **AUTOMATICALLY** try alternative searches
4. **LOG** every decision for final review
5. **CONTINUE** with remaining papers regardless of failures
6. **REPORT** all actions in the introduction PDF

## Quality Assurance

The system must:
- Process ALL citations without stopping
- Exclude ALL unverifiable papers
- Try at least 3 alternative search strategies per failure
- Maintain detailed logs of all decisions
- Generate comprehensive report for user review
- Achieve >95% success rate or flag for manual review

All verification decisions are final and will be reviewed by the user only in the completed introduction.pdf.

## ðŸ” Pattern Detection and Circuit Breakers

### Suspicious Pattern Detection

Monitor for these patterns across batches to detect systematic problems:

```python
class PatternDetector:
    def __init__(self):
        self.failure_history = []
        self.author_frequency = Counter()
        self.error_patterns = Counter()
        self.source_failures = Counter()
        
    def analyze_patterns(self, recent_papers):
        """Detect suspicious patterns in recent verifications"""
        patterns_detected = []
        
        # Pattern 1: Same author appearing too frequently
        for author, count in self.author_frequency.most_common(5):
            if count > len(recent_papers) * 0.3:  # >30% of papers
                patterns_detected.append({
                    'type': 'AUTHOR_DOMINANCE',
                    'detail': f"Author '{author}' in {count} papers",
                    'severity': 'HIGH'
                })
        
        # Pattern 2: Repeated error types
        for error, count in self.error_patterns.most_common(3):
            if count >= 5:
                patterns_detected.append({
                    'type': 'REPEATED_ERROR',
                    'detail': f"Error '{error}' occurred {count} times",
                    'severity': 'MEDIUM'
                })
        
        # Pattern 3: Source-specific failures
        for source, failures in self.source_failures.items():
            if failures > 5:
                patterns_detected.append({
                    'type': 'SOURCE_PROBLEM',
                    'detail': f"Source '{source}' has {failures} failures",
                    'severity': 'HIGH'
                })
        
        # Pattern 4: Title extraction problems
        title_issues = sum(1 for p in recent_papers 
                          if p.get('title', '').startswith(('Draft', 'IEEE', 'Page')))
        if title_issues > 3:
            patterns_detected.append({
                'type': 'TITLE_EXTRACTION_FAILURE',
                'detail': f"{title_issues} papers have header/footer as title",
                'severity': 'CRITICAL'
            })
        
        # Pattern 5: Unknown authors prevalence
        unknown_authors = sum(1 for p in recent_papers
                             if 'unknown' in str(p.get('authors', '')).lower())
        if unknown_authors > 2:
            patterns_detected.append({
                'type': 'UNKNOWN_AUTHORS_EPIDEMIC',
                'detail': f"{unknown_authors} papers have unknown authors",
                'severity': 'CRITICAL'
            })
        
        return patterns_detected
```

### Circuit Breaker Activation

```python
class CircuitBreaker:
    def __init__(self):
        self.consecutive_failures = 0
        self.total_failures = 0
        self.total_attempts = 0
        self.failure_threshold = 0.25  # 25% failure rate
        self.consecutive_limit = 5
        
    def record_attempt(self, success):
        """Record verification attempt and check circuit breakers"""
        self.total_attempts += 1
        
        if not success:
            self.consecutive_failures += 1
            self.total_failures += 1
        else:
            self.consecutive_failures = 0  # Reset on success
        
        # Check circuit breaker conditions
        breaker_triggered, reason = self.check_breakers()
        if breaker_triggered:
            self.trigger_emergency_stop(reason)
        
    def check_breakers(self):
        """Check if any circuit breaker conditions are met"""
        
        # Breaker 1: Consecutive failures
        if self.consecutive_failures >= self.consecutive_limit:
            return True, f"CONSECUTIVE_FAILURES: {self.consecutive_failures} in a row"
        
        # Breaker 2: Overall failure rate
        if self.total_attempts > 10:
            failure_rate = self.total_failures / self.total_attempts
            if failure_rate > self.failure_threshold:
                return True, f"HIGH_FAILURE_RATE: {failure_rate*100:.1f}%"
        
        # Breaker 3: Specific error patterns
        if self.detect_systematic_error():
            return True, "SYSTEMATIC_ERROR: Same error repeating"
        
        return False, None
    
    def trigger_emergency_stop(self, reason):
        """Emergency stop when circuit breaker triggers"""
        
        # Create critical alert
        alert = {
            'timestamp': datetime.now().isoformat(),
            'reason': reason,
            'stats': {
                'total_attempts': self.total_attempts,
                'total_failures': self.total_failures,
                'consecutive_failures': self.consecutive_failures
            },
            'action': 'EMERGENCY_STOP'
        }
        
        # Save alert
        with open('output/CRITICAL_ALERTS.txt', 'a') as f:
            f.write(f"\n{'='*60}\n")
            f.write(f"CIRCUIT BREAKER TRIGGERED: {reason}\n")
            f.write(f"Time: {alert['timestamp']}\n")
            f.write(f"Stats: {alert['stats']}\n")
            f.write(f"{'='*60}\n")
        
        # Stop processing
        raise SystemExit(f"CIRCUIT BREAKER: {reason} - Manual intervention required")
```

### Pattern-Based Recovery Strategies

```python
def apply_pattern_based_recovery(pattern_type, details):
    """Apply specific recovery based on detected pattern"""
    
    recovery_strategies = {
        'AUTHOR_DOMINANCE': {
            'action': 'diversify_search',
            'method': lambda: add_exclusion_filter('author', details['author'])
        },
        'REPEATED_ERROR': {
            'action': 'switch_extraction_method',
            'method': lambda: switch_to_alternative_extractor()
        },
        'SOURCE_PROBLEM': {
            'action': 'blacklist_source',
            'method': lambda: add_source_to_blacklist(details['source'])
        },
        'TITLE_EXTRACTION_FAILURE': {
            'action': 'enhanced_title_extraction',
            'method': lambda: enable_ocr_for_titles()
        },
        'UNKNOWN_AUTHORS_EPIDEMIC': {
            'action': 'strict_author_validation',
            'method': lambda: enable_strict_author_checks()
        }
    }
    
    if pattern_type in recovery_strategies:
        strategy = recovery_strategies[pattern_type]
        print(f"Applying recovery: {strategy['action']}")
        return strategy['method']()
    
    return None
```

### Emergency Manual Review Queue

```python
def add_to_emergency_review(paper_info, failure_details, patterns_detected):
    """Add paper to emergency manual review queue"""
    
    review_entry = {
        'timestamp': datetime.now().isoformat(),
        'paper': paper_info,
        'failure': failure_details,
        'patterns': patterns_detected,
        'priority': 'CRITICAL' if len(patterns_detected) > 2 else 'HIGH'
    }
    
    # Create human-readable report
    report = f"""
EMERGENCY MANUAL REVIEW REQUIRED
=================================
Paper: {paper_info.get('title', 'Unknown')}
Authors: {paper_info.get('authors', 'Unknown')}

Failure Details:
{json.dumps(failure_details, indent=2)}

Patterns Detected:
{json.dumps(patterns_detected, indent=2)}

Recommended Actions:
1. Check if extraction method needs updating
2. Verify source reliability
3. Consider manual download and verification
4. Update forbidden pattern lists if needed
=================================
"""
    
    # Save to emergency queue
    with open('output/EMERGENCY_REVIEW_QUEUE.txt', 'a') as f:
        f.write(report)
    
    # Also save structured data
    with open('output/emergency_review.jsonl', 'a') as f:
        f.write(json.dumps(review_entry) + '\n')
```

### Continuous Monitoring Dashboard

```python
def update_monitoring_dashboard():
    """Update real-time monitoring dashboard"""
    
    dashboard = {
        'last_updated': datetime.now().isoformat(),
        'health_status': 'HEALTHY',  # or 'WARNING' or 'CRITICAL'
        'metrics': {
            'success_rate_1h': calculate_recent_success_rate(hours=1),
            'success_rate_24h': calculate_recent_success_rate(hours=24),
            'active_circuit_breakers': check_active_breakers(),
            'pattern_alerts': get_active_pattern_alerts(),
            'papers_in_emergency_queue': count_emergency_queue()
        },
        'recent_failures': get_recent_failures(limit=5),
        'recommendations': generate_recommendations()
    }
    
    # Determine health status
    if dashboard['metrics']['success_rate_1h'] < 0.7:
        dashboard['health_status'] = 'CRITICAL'
    elif dashboard['metrics']['success_rate_1h'] < 0.9:
        dashboard['health_status'] = 'WARNING'
    
    # Save dashboard
    with open('output/verification_dashboard.json', 'w') as f:
        json.dump(dashboard, f, indent=2)
    
    # Print summary if critical
    if dashboard['health_status'] == 'CRITICAL':
        print("\nâš ï¸  CRITICAL: Verification system health degraded!")
        print(f"Success rate: {dashboard['metrics']['success_rate_1h']*100:.1f}%")
        print(f"Active alerts: {len(dashboard['metrics']['pattern_alerts'])}")
```