# Verification System Testing Protocol

## üéØ Purpose
This document provides mandatory pre-flight tests for the paper verification system. These tests MUST be run before ANY paper collection session to ensure the verification mechanisms are working correctly.

## ‚ö†Ô∏è CRITICAL: Test Before Trust
The presence of "Unknown Authors" and invalid titles in previous runs indicates systematic verification failures. These tests prevent such failures by validating the verification system itself.

## üß™ Mandatory Pre-Flight Tests

### Test 1: Known Good Paper
**Purpose**: Verify the system can correctly extract metadata from a valid paper

```python
def test_known_good_paper():
    """Test with a well-known, accessible paper"""
    test_paper = {
        'arxiv_id': '2201.11903',
        'expected_title': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models',
        'expected_authors': ['Jason Wei', 'Xuezhi Wang', 'Dale Schuurmans'],
        'expected_year': 2022
    }
    
    # Download and extract
    pdf_path = download_arxiv(test_paper['arxiv_id'])
    extracted = extract_paper_content(pdf_path)
    
    # Validate extraction
    assert similarity(extracted['title'], test_paper['expected_title']) > 0.9
    assert any(author in extracted['authors'] for author in test_paper['expected_authors'])
    assert abs(extracted['year'] - test_paper['expected_year']) <= 1
    
    return "PASS: Known good paper extracted correctly"
```

### Test 2: Injection Attack Test
**Purpose**: Ensure the system rejects papers with forbidden patterns

```python
def test_unknown_author_rejection():
    """Test that Unknown Author papers are rejected"""
    
    # Create test cases with forbidden patterns
    test_cases = [
        {'author': 'Unknown Author', 'title': 'Valid Title'},
        {'author': 'Author, Unknown', 'title': 'Another Title'},
        {'author': 'V. Author', 'title': 'Test Paper'},
        {'author': '[Author Name]', 'title': 'Placeholder Paper'},
        {'author': 'Anonymous', 'title': 'Anonymous Submission'}
    ]
    
    for test in test_cases:
        # Attempt to add to bibliography
        result = verify_paper_metadata(test)
        
        # Must be rejected
        assert result['status'] == 'REJECTED'
        assert 'Forbidden author pattern' in result['reason']
        
        # Must not appear in ref.bib
        assert test['author'] not in read_bibliography()
    
    return "PASS: All forbidden patterns correctly rejected"
```

### Test 3: Malformed PDF Test
**Purpose**: Test fallback extraction methods when primary extraction fails

```python
def test_malformed_pdf():
    """Test with PDFs that have extraction challenges"""
    
    test_pdfs = [
        'header_footer_only.pdf',  # PDF with only headers/footers
        'scanned_image.pdf',       # Scanned PDF requiring OCR
        'encrypted.pdf',           # Password-protected PDF
        'corrupted.pdf'            # Partially corrupted file
    ]
    
    for pdf in test_pdfs:
        # Primary extraction
        primary_result = extract_with_pdfplumber(pdf)
        
        if not primary_result['success']:
            # Must attempt secondary method
            secondary_result = extract_with_pymupdf(pdf)
            
            if not secondary_result['success']:
                # Must attempt tertiary method
                tertiary_result = extract_with_tika(pdf)
                
                if not tertiary_result['success']:
                    # Must log and exclude
                    assert pdf in get_excluded_papers_log()
        
        # Must not have empty metadata in bibliography
        assert 'Unknown Author' not in read_bibliography()
    
    return "PASS: Malformed PDFs handled correctly"
```

### Test 4: Title Extraction Validation
**Purpose**: Ensure titles are extracted from correct location, not headers

```python
def test_title_extraction():
    """Test that titles come from paper content, not headers/footers"""
    
    forbidden_title_patterns = [
        r'^\d{4}-\d{2}-\d{2}$',  # Just a date
        r'^Draft version',        # Draft header
        r'IEEE TRANSACTIONS',     # Journal header
        r'^[A-Z]+-[A-Z]+-\d+$',  # Report numbers
        r'^\d+$',                 # Just numbers
        r'^Page \d+$'             # Page numbers
    ]
    
    test_pdf = 'sample_with_headers.pdf'
    extracted_title = extract_title(test_pdf)
    
    # Check against forbidden patterns
    for pattern in forbidden_title_patterns:
        assert not re.match(pattern, extracted_title), \
            f"Title matches forbidden pattern: {pattern}"
    
    # Title must have reasonable length
    assert 10 < len(extracted_title) < 200
    
    return "PASS: Title extraction avoids headers/footers"
```

### Test 5: Batch Validation Test
**Purpose**: Test detection of systematic problems across batches

```python
def test_batch_validation():
    """Test that systematic problems trigger circuit breakers"""
    
    # Create batch with suspicious patterns
    suspicious_batch = []
    for i in range(20):
        suspicious_batch.append({
            'author': 'Unknown Author',  # Same author
            'title': f'Paper {i}',
            'year': 2023
        })
    
    # Process batch
    batch_result = process_paper_batch(suspicious_batch)
    
    # Circuit breaker must trigger
    assert batch_result['circuit_breaker_triggered'] == True
    assert batch_result['reason'] == 'Same author in >20% of papers'
    assert len(batch_result['processed']) == 0  # Nothing should be added
    
    # Must generate alert
    assert os.path.exists('CRITICAL_ALERTS.txt')
    
    return "PASS: Batch validation detects systematic issues"
```

## üìä Test Report Format

After running all tests, generate `verification_test_report.md`:

```markdown
# Verification System Test Report
Generated: [timestamp]

## Test Results Summary
Total Tests: 5
Passed: [count]
Failed: [count]

## Detailed Results

### Test 1: Known Good Paper
- Status: [PASS/FAIL]
- Expected: Correctly extract title, authors, year
- Actual: [what happened]
- Details: [specific values extracted]

### Test 2: Injection Attack Test
- Status: [PASS/FAIL]
- Expected: Reject all forbidden patterns
- Actual: [what happened]
- Rejected patterns: [list]

### Test 3: Malformed PDF Test
- Status: [PASS/FAIL]
- Expected: Use fallback methods, exclude if all fail
- Actual: [what happened]
- Extraction methods tried: [list]

### Test 4: Title Extraction Validation
- Status: [PASS/FAIL]
- Expected: Extract real title, not headers
- Actual: [extracted title]
- Pattern checks: [pass/fail for each]

### Test 5: Batch Validation Test
- Status: [PASS/FAIL]
- Expected: Trigger circuit breaker
- Actual: [what happened]
- Alert generated: [yes/no]

## Verification System Status
[‚úì or ‚úó] Ready for production use

## Recommendations
[Any issues found and how to fix them]
```

## üö¶ Pre-Flight Checklist

Before starting paper collection:

```python
def run_preflight_checks():
    """Run all verification tests before paper collection"""
    
    print("Running verification system tests...")
    
    tests = [
        test_known_good_paper,
        test_unknown_author_rejection,
        test_malformed_pdf,
        test_title_extraction,
        test_batch_validation
    ]
    
    results = []
    for test in tests:
        try:
            result = test()
            results.append({'test': test.__name__, 'status': 'PASS', 'message': result})
            print(f"‚úì {test.__name__}: PASS")
        except AssertionError as e:
            results.append({'test': test.__name__, 'status': 'FAIL', 'message': str(e)})
            print(f"‚úó {test.__name__}: FAIL - {e}")
    
    # Generate report
    generate_test_report(results)
    
    # Only proceed if all tests pass
    if all(r['status'] == 'PASS' for r in results):
        print("\n‚úì All tests passed! Verification system ready.")
        return True
    else:
        print("\n‚úó Some tests failed! DO NOT proceed with paper collection.")
        print("Fix the issues and re-run tests.")
        return False
```

## ‚ö†Ô∏è Critical Reminders

1. **NEVER skip these tests** - They prevent catastrophic verification failures
2. **Fix all failures before proceeding** - A failing test means the system will produce invalid results
3. **Document any test modifications** - If you need to adjust tests, document why
4. **Re-run after system updates** - Any changes to verification code require re-testing

## üîç Common Test Failures and Solutions

### Test 1 Fails (Known Good Paper)
- **Cause**: Network issues or ArXiv API changes
- **Solution**: Try alternative ArXiv ID or check network connection

### Test 2 Fails (Injection Attack)
- **Cause**: Verification bypassed or not implemented
- **Solution**: Check that forbidden pattern detection is active in verification pipeline

### Test 3 Fails (Malformed PDF)
- **Cause**: PDF extraction libraries not installed or configured
- **Solution**: Verify pdfplumber, pymupdf, and tika are installed and working

### Test 4 Fails (Title Extraction)
- **Cause**: Extraction grabbing wrong text regions
- **Solution**: Adjust extraction coordinates or use different extraction method

### Test 5 Fails (Batch Validation)
- **Cause**: Circuit breakers not implemented or threshold too high
- **Solution**: Implement batch monitoring and adjust thresholds

## üöÄ Integration with Main Workflow

```python
# In main paper collection script:
if not run_preflight_checks():
    raise SystemExit("Verification tests failed. Aborting paper collection.")

# Only continue if all tests passed
print("Starting paper collection with verified system...")
```

**REMEMBER**: These tests are your safety net against verification failures. Run them EVERY time!