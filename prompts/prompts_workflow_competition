# Mathematical Modeling Competition Workflow

## üöÄ Complete Workflow for MCM/ICM, HiMCM, and IMMC

### Important: Incremental PDF Compilation
This workflow now uses incremental section-by-section compilation. Each section is compiled into its own PDF for review before proceeding. See `prompts_incremental_workflow` for details.

### Prerequisites
- Ensure MCP tools are installed and configured
- Activate appropriate Python virtual environments
- Have competition problem PDF in `problem/` folder

### Phase 1: Initial Setup
1. **Identify Competition Type**
   - Ask user: "Which competition is this for? (MCM/ICM, HiMCM, or IMMC)"
   - Each has specific formatting requirements

2. **Copy Template Files**
   ```bash
   # For MCM/ICM
   cp templates/MCMICMLatexTemplate/mcmthesis.cls output/
   cp templates/MCMICMLatexTemplate/main.tex output/
   
   # For HiMCM
   cp templates/HiMCMLatexTemplate/himcm.cls output/
   cp templates/HiMCMLatexTemplate/main.tex output/
   
   # For IMMC
   cp templates/IMMCLatexTemplate/immc.cls output/
   cp templates/IMMCLatexTemplate/main.tex output/
   ```

3. **Read Problem Statement**
   - Use Read tool on `problem/*.pdf`
   - Extract key requirements and constraints
   - Identify data sources needed

### Phase 2: Research & Data Collection

#### Step 1: Gather & Verify Source Material
1. **Web Scraping for Papers**
   - Use `prompts/prompts_webScraping` guidelines
   - Search for minimum 50 arXiv papers + 30 journal papers
   - Include search terms: `filetype:pdf`
   - Verify each download with HTTP 200 response
   - Store in `output/papers/`

2. **MANDATORY Authenticity Verification**
   - Re-download each file with Playwright MCP
   - Compute SHA-256 checksums
   - Ensure checksums match
   - Remove any mismatched files
   - Create verification_log.txt
   - Generate verification_report.md
   - Double check downloaded PDFs match with bibliography entries
   - For undownloadable papers:
     * Use Playwright to verify on journal/conference website
     * Check title and authors match
     * Create undownloadable_papers_list.md
     * Include only verified genuine papers in bibliography

3. **CRITICAL: Content Verification with Kill Switches**
   **‚ö†Ô∏è AUTOMATIC PROCESS - NO USER INTERVENTION**
   
   For EVERY downloaded paper:
   - Extract title, authors, abstract from PDF
   - Calculate title similarity (must be ‚â•80%)
   - **KILL SWITCH**: If similarity <80%, DELETE and exclude
   - Check domain relevance using abstract keywords
   - **KILL SWITCH**: If wrong field, DELETE immediately
   - Verify arXiv ID matches if applicable
   - **KILL SWITCH**: If wrong ID, DELETE and exclude
   
   All failures are:
   - Automatically handled (no stopping for user)
   - Logged in verification_failures.log
   - Excluded from bibliography
   - Reported in introduction.pdf for final review

4. **Double-Check Before Proceeding**
   ```bash
   # Run the verification script
   ~/.venv/webScraping/bin/python output/final_verification.py
   
   # If it fails, DO NOT CONTINUE
   # Find more sources and repeat verification
   ```

4. **Data Collection**
   - Search for relevant datasets (CSV, JSON)
   - Document data sources with checksums
   - Store in `output/data/`
   - Verify data integrity

### Phase 2.5: Research Breakthrough Analysis (NEW)

**CRITICAL**: This phase transforms your paper from standard to innovative!

#### Step 1: Deep Literature Analysis
1. **Systematic Gap Identification**:
   - Use: `prompts/prompts_research_breakthrough`
   - Read ALL downloaded papers thoroughly
   - Create: `output/research_gaps_analysis.md`
   - Document 10+ specific limitations/weaknesses
   - Each gap must have supporting citations

2. **Innovation Development**:
   - Combine existing theories/algorithms in novel ways
   - Challenge fundamental assumptions
   - Apply cross-domain knowledge
   - Create: `output/breakthrough_proposal.md`
   - Include mathematical/algorithmic descriptions

3. **Validation Check**:
   - Create: `output/innovation_check.json`
   - Verify novelty score ‚â• 6/10
   - Ensure 3+ major gaps addressed
   - Confirm implementation feasibility

4. **Quality Gate - DO NOT PROCEED UNLESS**:
   - ‚úì 10+ research gaps documented with citations
   - ‚úì Novel approach addresses 3+ major gaps  
   - ‚úì Innovation mathematically/algorithmically described
   - ‚úì Expected improvements quantified (15-20% typical)
   - ‚úì Differentiated from existing work

**If quality gate fails**: Return to web scraping for more specialized papers on limitations, failures, and challenges.

### Phase 3: Paper Generation (INCREMENTAL WORKFLOW)

**IMPORTANT**: Each section is now compiled separately for review. You MUST stop and wait for approval after each PDF is created.

#### Step 1: Introduction Section
1. **Generate Content**:
   - File: `output/introduction.tex`
   - Use: `prompts/prompts_introduction`
   - Include: Problem background, literature review, objectives
   
2. **Create Section PDF**:
   - Extract citations to `introduction_refs.bib`
   - Create `introduction_wrapper.tex` with template preamble
   - Compile `introduction_v1.pdf`
   
3. **STOP - Review Checkpoint**:
   - Display review checklist from `prompts_review_checkpoint`
   - Wait for user approval or revision requests
   - If revisions needed, create v2, v3, etc.

#### Step 2: Infographic Design
1. **After Introduction Approval**:
   - Use: `prompts/prompts_infographicDesign`
   - Create: `output/figures/infographic.png`
   
2. **STOP - Review Checkpoint**:
   - Show infographic for approval
   - Wait for user response

#### Step 3: Methods Section
1. **Generate Content**:
   - File: `output/methods.tex`
   - Use: `prompts/prompts_methods`
   - Include: Models, algorithms, architecture diagrams
   
2. **Create Section PDF**:
   - Extract citations to `methods_refs.bib`
   - Compile `methods_v1.pdf`
   
3. **STOP - Review Checkpoint**:
   - Display methods checklist
   - Wait for approval

#### Step 4: Code Development
1. **After Methods Approval**:
   - Generate Python scripts in `output/codes/`
   - Test all scripts with --test flag
   
2. **STOP - Code Review**:
   - Display .py files directly (no PDF)
   - Show summary of what each script does
   - Wait for approval before continuing

#### Step 5: Code Execution Handoff
1. **After Code Approval**:
   - Use: `prompts/prompts_code_execution_instructions`
   - Provide detailed execution instructions
   - Include time estimates and resource requirements
   
2. **STOP - Execution Handoff**:
   - User executes scripts on their system
   - Wait for execution completion
   - User provides outputs (figures/data)

#### Step 6: Results and Discussion
1. **After Receiving Execution Outputs**:
   - Use user-provided figures/data
   - Write `resultsAndDiscussions.tex`
   
2. **Create Section PDF**:
   - Include all figures and tables
   - Compile `results_v1.pdf`
   
3. **STOP - Review Checkpoint**:
   - Display results checklist
   - Verify all visualizations

#### Step 7: Conclusions Section
1. **Generate Content**:
   - File: `output/conclusions.tex`
   - Use: `prompts/prompts_conclusions`
   
2. **Create Section PDF**:
   - Compile `conclusions_v1.pdf`
   
3. **STOP - Review Checkpoint**:
   - Display conclusions checklist

#### Step 8: Competition-Specific Sections
1. **Executive Summary**:
   - File: `output/summary.tex`
   - Length: Exactly 550 words
   - Compile: `summary_v1.pdf`
   - STOP for review
   
2. **Letter to Decision Makers**:
   - File: `output/letter.tex`
   - Length: 600-700 words
   - Compile: `letter_v1.pdf`
   - STOP for review

#### Step 9: Appendices
1. **Generate MANDATORY Appendices**:
   - Code Documentation: `appendixCodes.tex`
   - AI Usage Report: `appendixAIReport.tex` (REQUIRED for competition compliance)
   
2. **Generate OPTIONAL General Appendix** (if needed):
   - General Appendix: `appendix.tex` (ONLY if you have supplementary material)
   - Examples: Extended results, additional figures, detailed derivations
   - See `prompts/prompts_appendix` for what belongs here vs AI Report
   
3. **CRITICAL DISTINCTION**:
   - `appendixAIReport.tex` = MANDATORY ethics disclosure (AI tool usage)
   - `appendix.tex` = OPTIONAL technical supplement (extra results/figures)
   - DO NOT confuse these two!
   
4. **Create Combined PDF**:
   - Compile: `appendices_v1.pdf`
   - STOP for review

### Phase 4: Final Assembly (ONLY AFTER ALL SECTIONS APPROVED)

#### Step 1: Verify All Sections Approved
- Check that all section PDFs have been approved
- Ensure latest versions are being used
- Confirm no pending revision requests
- Verify all .tex files exist and are complete

#### Step 2: Combine All Content and Bibliography
1. **Merge Section Bibliographies**:
   ```bash
   # Combine all section-specific .bib files
   cd output
   ~/.venv/ml_31123121/bin/python ../utilityScripts/merge_bibliographies.py \
     --output ref_final.bib
   
   # Verify the merge
   echo "Merged bibliography contains $(grep -c '@' ref_final.bib) entries"
   ```

2. **Update main.tex to include ALL sections**:
   ```latex
   % Include all approved section content
   \input{summary}
   \input{letter}
   \input{introduction}
   \input{methods}
   \input{resultsAndDiscussions}
   \input{conclusions}
   \appendix
   \input{appendixCodes}
   \input{appendixAIReport}
   % \input{appendix}  % Uncomment ONLY if you have general appendix content
   
   % Use the merged bibliography
   \bibliography{ref_final}  % NOT ref.bib or individual section files
   ```

**CRITICAL**: The final assembly includes:
- All section content files (.tex) that were already written and approved
- All figures and tables referenced in the sections
- The merged bibliography containing all citations from all sections
- NO rewriting or regeneration of content - just assembly!

#### Step 3: Final Compilation

1. **Run Comprehensive Check Script**
   ```bash
   # Create and run pre_compilation_check.py
   cat > output/pre_compilation_check.py << 'EOF'
   import os
   import re
   import json
   from pathlib import Path
   
   def check_all_systems():
       errors = []
       
       # Check all tex files exist
       required_files = ['summary.tex', 'letter.tex', 'introduction.tex', 
                        'methods.tex', 'resultsAndDiscussions.tex', 
                        'conclusions.tex', 'appendixCodes.tex', 
                        'appendixAIReport.tex']  # Note: appendix.tex is OPTIONAL
       
       for file in required_files:
           if not Path(f"output/{file}").exists():
               errors.append(f"Missing: {file}")
       
       # Check word counts
       summary_text = Path("output/summary.tex").read_text()
       word_count = len(re.findall(r'\b\w+\b', summary_text))
       if not (540 <= word_count <= 560):
           errors.append(f"Summary word count: {word_count} (should be 550)")
       
       # Check content verification was performed
       if not Path("output/paper_metadata.json").exists():
           errors.append("CRITICAL: No content verification performed - paper_metadata.json missing")
       
       if not Path("output/verification_db.json").exists():
           errors.append("CRITICAL: No verification database - verification_db.json missing")
       else:
           # Check verification success rate
           with open("output/verification_db.json", 'r') as f:
               vdb = json.load(f)
           success_rate = vdb.get('verified', 0) / vdb.get('total_papers', 1)
           if success_rate < 0.95:
               errors.append(f"Verification success rate too low: {success_rate:.1%}")
       
       # Check all citations have PDFs
       bib_content = Path("output/ref.bib").read_text()
       citations = re.findall(r'@\w+{([^,]+),', bib_content)
       
       # Check if undownloadable papers list exists
       verified_undownloadable = []
       if Path("output/undownloadable_papers_list.md").exists():
           undownloadable_content = Path("output/undownloadable_papers_list.md").read_text()
           # Extract verified citations from the list
           for cite in citations:
               if cite in undownloadable_content and "‚úÖ Verified Genuine" in undownloadable_content:
                   verified_undownloadable.append(cite)
       
       for cite in citations:
           pdf_exists = any(Path("output/papers").rglob(f"*{cite}*.pdf"))
           if not pdf_exists and cite not in verified_undownloadable:
               errors.append(f"No PDF for citation: {cite} (not in verified undownloadable list)")
       
       # Check forbidden words
       forbidden = ['Innovative', 'Meticulous', 'Intricate', 'Notable', 
                   'Versatile', 'Noteworthy', 'Invaluable', 'Pivotal']
       
       for tex_file in Path("output").glob("*.tex"):
           content = tex_file.read_text()
           for word in forbidden:
               if word.lower() in content.lower():
                   errors.append(f"Forbidden word '{word}' in {tex_file.name}")
       
       # Check figures and data
       fig_refs = re.findall(r'\\includegraphics.*{figures/([^}]+)}', 
                            Path("output").read_text())
       for fig in fig_refs:
           if not Path(f"output/figures/{fig}").exists():
               errors.append(f"Missing figure: {fig}")
       
       if errors:
           print("‚ùå ERRORS FOUND - DO NOT COMPILE:")
           for error in errors:
               print(f"  - {error}")
           return False
       
       print("‚úÖ All pre-compilation checks passed!")
       return True
   
   if __name__ == "__main__":
       if not check_all_systems():
           exit(1)
   EOF
   
   # Run the check
   ~/.venv/ml_31123121/bin/python output/pre_compilation_check.py
   ```

2. **Generate Title Suggestions**
   - Follow `prompts/prompts_title_generation` to create title options
   - Generate `output/title_suggestions.md` with 5-7 analyzed options
   - Select the best title that highlights your breakthrough approach
   - Example: "Hybrid Neural-Evolutionary Optimization for Urban Traffic Flow: A Multi-Scale Approach"

3. **ONLY IF ALL CHECKS PASS - Update main.tex**
   - First, set the title from your selected option:
   ```latex
   \title{Your Selected Title from title_suggestions.md}
   ```
   - Then include all sections:
   ```latex
   \input{summary}
   \input{letter}
   \input{introduction}
   \input{methods}
   \input{resultsAndDiscussions}
   \input{conclusions}
   \appendix
   \input{appendixCodes}
   \input{appendixAIReport}
   % \input{appendix}  % Uncomment ONLY if you have general appendix content
   ```

4. **Compile PDF**
   ```bash
   cd output
   pdflatex main.tex
   biber main
   pdflatex main.tex
   pdflatex main.tex
   ```

5. **Post-Compilation Verification**
   - Check for LaTeX errors: `grep -i error main.log`
   - Verify PDF page count
   - Confirm all sections included
   - Test all hyperlinks work

### Important Competition Rules

1. **No Author Information**
   - Never include names or affiliations
   - Use only team control number

2. **Page Limits**
   - Summary: Exactly 1 page
   - Letter: 1-2 pages
   - Total paper: Check competition guidelines

3. **Citation Style**
   - Always use IEEE numeric style
   - Configure: `\usepackage[backend=biber,style=ieee]{biblatex}`

4. **Required Sections**
   - All competitions require summary and main paper
   - Letter required for policy-oriented problems
   - AI usage disclosure increasingly important

### Quality Checklist
- [ ] Problem clearly restated in introduction
- [ ] Methods well-documented with equations
- [ ] Results supported by data and visualizations
- [ ] Conclusions answer the problem questions
- [ ] All citations verified and accessible
- [ ] Page limits strictly adhered to
- [ ] No author information included
- [ ] AI usage properly documented