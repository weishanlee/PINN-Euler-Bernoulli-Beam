# Final Comprehensive Double-Check Protocol

## üö® CRITICAL: Pre-Compilation Master Verification

**This is the FINAL checkpoint. DO NOT compile the PDF until ALL checks pass.**

## üìã Master Verification Script

Create and run this comprehensive verification script:

```python
# master_verification.py
import os
import re
import json
import hashlib
from pathlib import Path
from datetime import datetime

class MasterVerification:
    def __init__(self):
        self.errors = []
        self.warnings = []
        self.report = {
            "timestamp": datetime.now().isoformat(),
            "checks_performed": [],
            "errors": [],
            "warnings": [],
            "status": "PENDING"
        }
    
    def check_web_scraping_integrity(self):
        """Verify all web-scraped materials are genuine"""
        print("üîç Checking web scraping integrity...")
        
        # Check verification report exists
        if not Path("output/verification_report.md").exists():
            self.errors.append("CRITICAL: No web scraping verification report found")
            return False
        
        # Check paper counts
        arxiv_count = len(list(Path("output/papers/arxiv_papers").glob("*.pdf")))
        journal_count = len(list(Path("output/papers/journal_papers").glob("*.pdf")))
        
        if arxiv_count < 50:
            self.errors.append(f"Insufficient arXiv papers: {arxiv_count} < 50")
        if journal_count < 30:
            self.errors.append(f"Insufficient journal papers: {journal_count} < 30")
        
        # Verify checksums log exists
        if not Path("output/verification_log.txt").exists():
            self.errors.append("No verification log found")
        
        self.report["checks_performed"].append("web_scraping_integrity")
        return len(self.errors) == 0
    
    def check_all_sections_exist(self):
        """Verify all required LaTeX sections are present"""
        print("üìÑ Checking all sections exist...")
        
        required_sections = {
            "competition": ["summary.tex", "letter.tex", "introduction.tex", 
                          "methods.tex", "resultsAndDiscussions.tex", 
                          "conclusions.tex", "appendixCodes.tex", 
                          "appendixAIReport.tex"],
            "journal": ["abstract.tex", "introduction.tex", "methods.tex",
                       "resultsAndDiscussions.tex", "conclusions.tex",
                       "appendixAIReport.tex"]
        }
        
        # Determine paper type
        paper_type = "competition" if Path("output/summary.tex").exists() else "journal"
        
        for section in required_sections[paper_type]:
            section_path = Path(f"output/{section}")
            if not section_path.exists():
                self.errors.append(f"Missing section: {section}")
            elif section_path.stat().st_size < 100:
                self.warnings.append(f"Section suspiciously small: {section}")
        
        self.report["checks_performed"].append("section_existence")
        return True
    
    def check_section_summary_exists(self):
        """Verify final section summary was created and reviewed"""
        print("üìù Checking section summary...")
        
        # Check if final_sections_summary.md exists
        if not Path("output/final_sections_summary.md").exists():
            self.errors.append("CRITICAL: final_sections_summary.md not found - section files not summarized!")
            return False
        
        # Check if section detection report exists
        if not Path("output/section_detection_report.json").exists():
            self.errors.append("CRITICAL: section_detection_report.json not found - version detection not performed!")
            return False
        
        # Verify summary has content
        summary_content = Path("output/final_sections_summary.md").read_text()
        if len(summary_content) < 500:
            self.errors.append("Section summary file is suspiciously small")
        
        # Check if summary includes all expected sections
        required_keywords = ["Sections Used in Final main.pdf", "Bibliography Files", "Supporting Files"]
        for keyword in required_keywords:
            if keyword not in summary_content:
                self.warnings.append(f"Section summary missing: {keyword}")
        
        # Load detection report to verify it was used
        try:
            with open("output/section_detection_report.json", 'r') as f:
                detection_report = json.load(f)
            if 'detected_sections' not in detection_report:
                self.errors.append("Section detection report is malformed")
        except Exception as e:
            self.errors.append(f"Failed to load section detection report: {str(e)}")
        
        print(f"  Summary file size: {len(summary_content)} characters")
        self.report["checks_performed"].append("section_summary_verification")
        return len(self.errors) == 0
    
    def check_deep_content_verification(self):
        """Perform deep content verification on all papers"""
        print("üî¨ Performing deep content verification...")
        
        # Check paper_metadata.json exists
        if not Path("output/paper_metadata.json").exists():
            self.errors.append("CRITICAL: paper_metadata.json not found - no content verification performed")
            return False
        
        # Check verification_db.json exists
        if not Path("output/verification_db.json").exists():
            self.errors.append("CRITICAL: verification_db.json not found - no verification database")
            return False
        
        # Load verification data
        with open("output/paper_metadata.json", 'r') as f:
            metadata = json.load(f)
        
        with open("output/verification_db.json", 'r') as f:
            verification_db = json.load(f)
        
        # Check verification statistics
        total_papers = verification_db.get('total_papers', 0)
        verified = verification_db.get('verified', 0)
        failed = verification_db.get('failed', 0)
        
        if total_papers == 0:
            self.errors.append("No papers were verified")
            return False
        
        success_rate = verified / total_papers
        print(f"  Verification success rate: {success_rate:.1%}")
        
        if success_rate < 0.95:
            self.errors.append(f"Verification success rate too low: {success_rate:.1%} < 95%")
        
        # Check for title mismatches
        title_mismatches = 0
        domain_mismatches = 0
        
        for citation_key, data in metadata.items():
            if 'similarity_score' in data and data['similarity_score'] < 0.8:
                title_mismatches += 1
            if 'verification_status' in data and data['verification_status'] == 'FAILED':
                if 'domain_mismatch' in str(data):
                    domain_mismatches += 1
        
        if title_mismatches > 0:
            self.errors.append(f"{title_mismatches} papers have title similarity < 80%")
        
        if domain_mismatches > 0:
            self.errors.append(f"{domain_mismatches} papers are from wrong domain/field")
        
        # Check verification failures log
        if Path("output/verification_failures.log").exists():
            with open("output/verification_failures.log", 'r') as f:
                failures = f.readlines()
            if len(failures) > 10:
                self.warnings.append(f"High number of verification failures: {len(failures)}")
        
        self.report["checks_performed"].append("deep_content_verification")
        return len(self.errors) == 0
    
    def check_citation_integrity(self):
        """Verify all citations have corresponding PDFs"""
        print("üìö Checking citation integrity...")
        
        # Extract all citations from all tex files
        all_citations = set()
        for tex_file in Path("output").glob("*.tex"):
            content = tex_file.read_text()
            citations = re.findall(r'\\cite{([^}]+)}', content)
            for cite_group in citations:
                keys = [k.strip() for k in cite_group.split(',')]
                all_citations.update(keys)
        
        # Check bibliography file
        bib_entries = {}
        unknown_authors_found = []
        if Path("output/ref.bib").exists():
            bib_content = Path("output/ref.bib").read_text()
            # Extract all entries with their full content
            entry_pattern = r'@\w+{([^,]+),.*?(?=@|\Z)'
            entries_full = re.findall(entry_pattern, bib_content, re.DOTALL)
            
            # Also get the full entry content to check for unknown authors
            full_entries = re.findall(r'(@\w+{[^,]+,.*?)(?=@|\Z)', bib_content, re.DOTALL)
            
            for i, entry_key in enumerate(entries_full):
                bib_entries[entry_key.strip()] = True
                
                # Check for Unknown Authors in this entry
                if i < len(full_entries):
                    entry_content = full_entries[i].lower()
                    if any(term in entry_content for term in ["unknown author", "unknown authors", "authors unknown", "anonymous"]):
                        unknown_authors_found.append(entry_key.strip())
                        self.errors.append(f"CRITICAL: 'Unknown Authors' found in citation '{entry_key.strip()}' - STRICTLY FORBIDDEN")
        
        # Check each citation
        missing_pdfs = []
        citation_pdf_matches = {}
        
        for cite in all_citations:
            pdf_found = False
            matching_pdf = None
            
            # Check if citation exists in bibliography
            if cite not in bib_entries:
                self.errors.append(f"Citation '{cite}' not found in ref.bib")
                continue
            
            # Look for corresponding PDF
            for pdf in Path("output/papers").rglob("*.pdf"):
                if cite.lower() in pdf.name.lower():
                    pdf_found = True
                    matching_pdf = pdf.name
                    break
            
            if not pdf_found:
                missing_pdfs.append(cite)
            else:
                citation_pdf_matches[cite] = matching_pdf
        
        # Double-check PDF content matches bibliography
        if Path("output/undownloadable_papers_list.md").exists():
            # Check if missing PDFs are in the verified undownloadable list
            undownloadable_content = Path("output/undownloadable_papers_list.md").read_text()
            verified_undownloadable = []
            
            for cite in missing_pdfs[:]:  # Create a copy to iterate
                if cite in undownloadable_content and "‚úÖ Verified Genuine" in undownloadable_content:
                    verified_undownloadable.append(cite)
                    missing_pdfs.remove(cite)
        
        if missing_pdfs:
            self.errors.append(f"Citations without PDFs (not in verified list): {', '.join(missing_pdfs)}")
        
        if verified_undownloadable:
            self.warnings.append(f"Citations verified but PDFs unavailable: {', '.join(verified_undownloadable)}")
        
        # Report matches found
        print(f"  Found {len(citation_pdf_matches)} citation-PDF matches")
        print(f"  {len(verified_undownloadable)} verified but unavailable")
        print(f"  {len(missing_pdfs)} missing without verification")
        
        # Report Unknown Authors violations
        if unknown_authors_found:
            print(f"  ‚ùå CRITICAL: {len(unknown_authors_found)} citations with 'Unknown Authors' found")
            print(f"  These MUST be removed: {', '.join(unknown_authors_found)}")
        
        self.report["checks_performed"].append("citation_integrity")
        return len(missing_pdfs) == 0 and len(unknown_authors_found) == 0
    
    def check_bibliography_sanity(self):
        """Check bibliography for suspicious patterns"""
        print("üìö Checking bibliography sanity...")
        
        from collections import Counter
        import bibtexparser
        
        bib_file = Path("output/ref.bib")
        if not bib_file.exists():
            self.errors.append("CRITICAL: ref.bib not found")
            return False
        
        # Parse bibliography
        with open(bib_file, 'r', encoding='utf-8') as f:
            bib_database = bibtexparser.load(f)
        
        entries = bib_database.entries
        total_entries = len(entries)
        
        if total_entries == 0:
            self.errors.append("CRITICAL: Bibliography is empty")
            return False
        
        # Extract all authors
        all_authors = []
        invalid_authors = []
        suspicious_titles = []
        
        for entry in entries:
            # Check authors
            authors = entry.get('author', '')
            
            # Check for forbidden author patterns
            forbidden_patterns = [
                'Unknown Author', 'Author, Unknown', 'Authors Unknown',
                'Anonymous', 'N/A', 'TBD', '[Author Name]'
            ]
            
            if any(pattern in authors for pattern in forbidden_patterns):
                invalid_authors.append({
                    'key': entry.get('ID', 'unknown'),
                    'authors': authors
                })
            
            # Extract individual authors
            if authors and 'Unknown' not in authors:
                # Split by 'and' and clean
                author_list = [a.strip() for a in authors.split(' and ')]
                all_authors.extend(author_list)
            
            # Check titles
            title = entry.get('title', '')
            if len(title) < 10 or any(p in title for p in ['Draft version', 'Template', '[Title]']):
                suspicious_titles.append({
                    'key': entry.get('ID', 'unknown'),
                    'title': title
                })
        
        # Count author frequencies
        author_counts = Counter(all_authors)
        
        # Check for suspicious patterns
        sanity_issues = []
        
        # Pattern 1: Author domination
        for author, count in author_counts.most_common(5):
            if count > total_entries * 0.2:  # Same author in >20% of papers
                sanity_issues.append(f"Author '{author}' appears in {count}/{total_entries} papers ({count/total_entries*100:.1f}%)")
        
        # Pattern 2: Invalid authors
        if invalid_authors:
            self.errors.append(f"CRITICAL: {len(invalid_authors)} entries with forbidden author patterns")
            for entry in invalid_authors[:5]:  # Show first 5
                print(f"  - {entry['key']}: {entry['authors']}")
        
        # Pattern 3: Suspicious titles
        if suspicious_titles:
            self.warnings.append(f"WARNING: {len(suspicious_titles)} entries with suspicious titles")
            for entry in suspicious_titles[:5]:  # Show first 5
                print(f"  - {entry['key']}: {entry['title']}")
        
        # Pattern 4: Year distribution
        years = [int(entry.get('year', 0)) for entry in entries if entry.get('year', '').isdigit()]
        if years:
            year_counts = Counter(years)
            for year, count in year_counts.most_common(3):
                if count > total_entries * 0.3:  # >30% from same year
                    sanity_issues.append(f"Year {year} has {count}/{total_entries} papers ({count/total_entries*100:.1f}%)")
        
        # Pattern 5: Journal concentration
        journals = [entry.get('journal', '') for entry in entries if entry.get('journal')]
        if journals:
            journal_counts = Counter(journals)
            for journal, count in journal_counts.most_common(3):
                if count > total_entries * 0.2:  # >20% from same journal
                    self.warnings.append(f"Journal '{journal}' has {count}/{total_entries} papers ({count/total_entries*100:.1f}%)")
        
        # Report findings
        if sanity_issues:
            print("‚ö†Ô∏è  Suspicious patterns detected:")
            for issue in sanity_issues:
                print(f"  - {issue}")
        
        # Generate sanity report
        sanity_report = {
            'total_entries': total_entries,
            'invalid_authors': len(invalid_authors),
            'suspicious_titles': len(suspicious_titles),
            'author_concentration': sanity_issues,
            'status': 'FAILED' if invalid_authors else 'WARNING' if sanity_issues else 'PASSED'
        }
        
        # Save report
        with open('output/bibliography_sanity_report.json', 'w') as f:
            json.dump(sanity_report, f, indent=2)
        
        self.report["checks_performed"].append("bibliography_sanity")
        
        # Fail if any invalid authors found
        if invalid_authors:
            return False
        
        return True
    
    def check_forbidden_words(self):
        """Check for AI-detection trigger words"""
        print("üö´ Checking for forbidden words...")
        
        forbidden = ['Innovative', 'Meticulous', 'Intricate', 'Notable',
                    'Versatile', 'Noteworthy', 'Invaluable', 'Pivotal',
                    'Potent', 'Fresh', 'Ingenious', 'Meticulously',
                    'Reportedly', 'Lucidly', 'Innovatively', 'Aptly',
                    'Methodically', 'Excellently', 'Compellingly',
                    'Impressively', 'Undoubtedly', 'Scholarly', 'Strategically']
        
        found_words = {}
        for tex_file in Path("output").glob("*.tex"):
            content = tex_file.read_text()
            for word in forbidden:
                if word.lower() in content.lower():
                    if tex_file.name not in found_words:
                        found_words[tex_file.name] = []
                    found_words[tex_file.name].append(word)
        
        if found_words:
            for file, words in found_words.items():
                self.errors.append(f"Forbidden words in {file}: {', '.join(words)}")
        
        self.report["checks_performed"].append("forbidden_words")
        return len(found_words) == 0
    
    def check_code_functionality(self):
        """Verify all code runs without errors"""
        print("üíª Checking code functionality...")
        
        import subprocess
        
        failed_scripts = []
        for script in Path("output/codes").glob("*.py"):
            try:
                result = subprocess.run(
                    ["~/.venv/ml_31123121/bin/python", str(script), "--test"],
                    capture_output=True, text=True, timeout=30
                )
                if result.returncode != 0:
                    failed_scripts.append(script.name)
            except Exception as e:
                failed_scripts.append(f"{script.name}: {str(e)}")
        
        if failed_scripts:
            self.errors.append(f"Failed scripts: {', '.join(failed_scripts)}")
        
        self.report["checks_performed"].append("code_functionality")
        return len(failed_scripts) == 0
    
    def check_figures_and_data(self):
        """Verify all referenced figures and data exist"""
        print("üìä Checking figures and data...")
        
        # Extract figure references
        figure_refs = set()
        for tex_file in Path("output").glob("*.tex"):
            content = tex_file.read_text()
            refs = re.findall(r'\\includegraphics.*{figures/([^}]+)}', content)
            figure_refs.update(refs)
        
        # Check figures exist
        missing_figures = []
        for fig in figure_refs:
            if not Path(f"output/figures/{fig}").exists():
                missing_figures.append(fig)
        
        if missing_figures:
            self.errors.append(f"Missing figures: {', '.join(missing_figures)}")
        
        # Check data documentation
        undocumented_data = []
        for csv in Path("output/data").glob("*.csv"):
            doc_file = csv.with_suffix('.txt')
            if not doc_file.exists():
                undocumented_data.append(csv.name)
        
        if undocumented_data:
            self.warnings.append(f"Undocumented data: {', '.join(undocumented_data)}")
        
        self.report["checks_performed"].append("figures_and_data")
        return len(missing_figures) == 0
    
    def check_word_limits(self):
        """Verify sections meet word/page requirements"""
        print("üìè Checking word limits...")
        
        # Check summary word count (competition only)
        if Path("output/summary.tex").exists():
            content = Path("output/summary.tex").read_text()
            word_count = len(re.findall(r'\b\w+\b', content))
            if not (540 <= word_count <= 560):
                self.errors.append(f"Summary word count {word_count} not in range 540-560")
        
        # Check abstract (journal only)
        if Path("output/abstract.tex").exists():
            content = Path("output/abstract.tex").read_text()
            word_count = len(re.findall(r'\b\w+\b', content))
            if word_count > 350:
                self.warnings.append(f"Abstract word count {word_count} > 350")
        
        self.report["checks_performed"].append("word_limits")
        return True
    
    def generate_final_report(self):
        """Generate comprehensive verification report"""
        print("\nüìã Generating final report...")
        
        if self.errors:
            self.report["status"] = "FAILED"
            self.report["errors"] = self.errors
        else:
            self.report["status"] = "PASSED"
        
        self.report["warnings"] = self.warnings
        
        # Save report
        with open("output/final_verification_report.json", 'w') as f:
            json.dump(self.report, f, indent=2)
        
        # Display results
        print("\n" + "="*60)
        print("FINAL VERIFICATION REPORT")
        print("="*60)
        
        if self.errors:
            print("\n‚ùå ERRORS FOUND - DO NOT COMPILE:")
            for error in self.errors:
                print(f"  ‚Ä¢ {error}")
        
        if self.warnings:
            print("\n‚ö†Ô∏è  WARNINGS:")
            for warning in self.warnings:
                print(f"  ‚Ä¢ {warning}")
        
        if not self.errors:
            print("\n‚úÖ ALL CHECKS PASSED - SAFE TO COMPILE")
        else:
            print("\nüõë FIX ALL ERRORS BEFORE PROCEEDING")
        
        print("="*60)
        
        return len(self.errors) == 0
    
    def run_all_checks(self):
        """Execute all verification checks"""
        checks = [
            self.check_web_scraping_integrity,
            self.check_deep_content_verification,  # NEW: Deep content check
            self.check_all_sections_exist,
            self.check_section_summary_exists,  # NEW: Verify section summary was created
            self.check_citation_integrity,
            self.check_bibliography_sanity,  # NEW: Check for suspicious bibliography patterns
            self.check_forbidden_words,
            self.check_code_functionality,
            self.check_figures_and_data,
            self.check_word_limits
        ]
        
        for check in checks:
            try:
                check()
            except Exception as e:
                self.errors.append(f"Check {check.__name__} failed: {str(e)}")
        
        return self.generate_final_report()

if __name__ == "__main__":
    verifier = MasterVerification()
    if not verifier.run_all_checks():
        exit(1)
    
    print("\nüéâ Ready for final compilation!")
```

## üîß Usage Instructions

1. **Save the script** to `output/master_verification.py`

2. **Run the verification**:
   ```bash
   cd output
   ~/.venv/ml_31123121/bin/python master_verification.py
   ```

3. **Interpret results**:
   - ‚úÖ **PASSED**: Safe to compile PDF
   - ‚ùå **FAILED**: Fix all errors before proceeding
   - ‚ö†Ô∏è **WARNINGS**: Review but not blocking

4. **Review the report**: Check `output/final_verification_report.json`

## üìä What Gets Checked

1. **Web Scraping Integrity**
   - Verification report exists
   - Sufficient papers downloaded
   - Checksums verified

2. **Section Completeness**
   - All required sections present
   - Sections have content

3. **Citation Integrity**
   - Every citation has a PDF
   - No orphaned references
   - NO "Unknown Authors" in any citation

4. **Language Quality**
   - No forbidden AI-detection words
   - Professional academic tone

5. **Code Functionality**
   - All scripts run without errors
   - Test mode passes

6. **Assets Availability**
   - All figures exist
   - All data documented

7. **Format Compliance**
   - Word counts within limits
   - Page restrictions met

## üö´ MANDATORY: Content Truthfulness Final Verification

**BEFORE FINAL COMPILATION, re-examine EVERY PARAGRAPH:**

### Paragraph-by-Paragraph Check Process
1. **Open each section file** (introduction.tex, methods.tex, etc.)
2. **For EVERY paragraph**, verify:
   - Each claim traces to a specific source (paper, data, or derivation)
   - Each result links to actual computation output
   - Each comparison has quantitative backing
   - Each conclusion follows from presented evidence
   - **NO fabricated data, straight-faced nonsense, bare-faced lies, or hallucinated statements**

### Red Flags to Check
- Overly specific numbers without source (e.g., "improved by 23.7%")
- Citations that seem too convenient or perfect
- Claims that sound impressive but vague
- Results that seem too good to be true
- Any statement you cannot trace to a source

### Kill Switch Protocol
**If ANY fabrication/nonsense/lies detected:**
1. ‚õî **STOP compilation immediately**
2. üîç **Go back to the section with issues**
3. üîÑ **Re-do the entire section with proper verification**
4. ‚úÖ **Compile new section PDF**
5. üë§ **Request fresh user approval before proceeding**

### Add Content Verification Check to master_verification.py

Add this method to the MasterVerification class:

```python
def check_content_truthfulness(self):
    """Verify every paragraph for factual accuracy"""
    print("üîç Checking content truthfulness...")
    
    sections = ["introduction.tex", "methods.tex", "resultsAndDiscussions.tex", 
                "conclusions.tex", "summary.tex", "abstract.tex"]
    
    fabrication_patterns = [
        r'improved by \d+\.\d+%(?! \()',  # Specific % without citation
        r'significantly (?:better|improved|outperformed)',  # Vague comparisons
        r'state-of-the-art',  # Often overused
        r'to the best of our knowledge',  # Hedge phrase
        r'it is well known that',  # Unsupported claim marker
    ]
    
    for section in sections:
        section_path = Path(f"output/{section}")
        if section_path.exists():
            content = section_path.read_text()
            
            # Check for unsupported claims
            for pattern in fabrication_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    self.warnings.append(f"Potential unsupported claim in {section}: {matches[0][:50]}...")
            
            # Check for verification comments
            verification_comments = len(re.findall(r'% (Source|Data source|Derivation|Based on):', content))
            total_paragraphs = len(re.findall(r'\n\n', content))
            
            if verification_comments < total_paragraphs * 0.5:
                self.warnings.append(f"Insufficient verification comments in {section}: {verification_comments} comments for ~{total_paragraphs} paragraphs")
    
    self.report["checks_performed"].append("content_truthfulness")
    return True
```

And add `self.check_content_truthfulness,` to the checks list in `run_all_checks()`.

## üõë FINAL CHECKPOINT

**DO NOT proceed to compilation unless:**
- Master verification shows **PASSED**
- All errors resolved
- Warnings reviewed and accepted
- Verification report saved
- **Every paragraph verified for truthfulness**
- **NO fabricated data or unsupported claims remain**

This is your last chance to ensure quality!

## üìã Final Pre-Compilation Checklist

**DO NOT compile PDF until ALL items checked:**

### Web Scraping & Verification
- [ ] All web-scraped materials verified twice
- [ ] All paper titles verified against PDF content
- [ ] No chemistry/biology/unrelated papers in bibliography
- [ ] NO "Unknown Authors" in ANY bibliography entry
- [ ] Content verification log reviewed and all PASSED
- [ ] Automatic relevance verification completed for ALL papers
- [ ] paper_metadata.json exists with all citations verified
- [ ] verification_db.json shows >95% success rate

### Content Requirements
- [ ] All sections follow word/page limits
- [ ] All files exist in correct folders
- [ ] No broken citations or references
- [ ] No prohibited words used (Innovative, Meticulous, etc.)
- [ ] **CRITICAL: Every paragraph verified for factual accuracy**
- [ ] **CRITICAL: NO fabricated data, straight-faced nonsense, bare-faced lies, or hallucinated statements**

### Code & Results
- [ ] Code has been executed by user and outputs provided
- [ ] Results are reproducible from provided code
- [ ] **CRITICAL: Code implementation MUST reflect actual methods described in tex file**
- [ ] **CRITICAL: No mismatch between mathematical formulas and code algorithms**

### Figures & Tables
- [ ] All figures exist in output/figures/
- [ ] All figures with boxes passed validation (no overlaps)
- [ ] Font sizes in all box diagrams ‚â• 8pt
- [ ] Box validation reports saved for all diagrams
- [ ] All tables have source data in output/data/
- [ ] Figures use approved color schemes

### LaTeX Compilation
- [ ] **CRITICAL: All LaTeX packages required by section content are included in main.tex**
- [ ] **CRITICAL: Package requirements have been verified using detect_required_packages.py**
- [ ] Author info correct (journals) or absent (competitions)

### External Reviews
- [ ] If user performed external ChatGPTO3ProReview, their review reports saved in output/review_reports/