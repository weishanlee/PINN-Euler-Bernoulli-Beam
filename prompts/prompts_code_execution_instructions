# Code Execution Instructions

## Overview
This document provides comprehensive guidelines for validating scripts and preparing execution instructions when Python scripts require user execution due to long runtime or resource constraints.

## ğŸ“‹ MANDATORY: Sequential Version Naming for All Scripts

### Version Naming Rules
1. **First creation**: Always use `_v1` suffix (e.g., `analysis_v1.py`)
2. **Any modification**: Create new version with incremented number (`_v2`, `_v3`...)
3. **Helper scripts**: Include version (`utils_v1.py`, `helpers_v1.py`)
4. **Output files**: Match script version (`results_v1.csv` from `analysis_v1.py`)
5. **NEVER use**: `_final`, `_updated`, `_revised`, `_new`

### Examples
```python
# âœ… CORRECT Version Progression
data_processing_v1.py    # Initial version
data_processing_v2.py    # Fixed bug in line 45
data_processing_v3.py    # Added progress tracking
model_training_v1.py     # Initial training script
model_training_v2.py     # Fixed timeout issue
model_training_v3.py     # Added checkpoint saving

# âŒ WRONG - Never use these
data_processing_final.py
model_training_updated.py
analysis_revised.py
script_new.py
```

### Output File Versioning
```python
# Script outputs must also be versioned
# From data_processing_v1.py:
output/data/cleaned_data_v1.csv
output/figures/plot_v1.png
output/models/model_weights_v1.pkl

# From data_processing_v2.py:
output/data/cleaned_data_v2.csv
output/figures/plot_v2.png
output/models/model_weights_v2.pkl
```

## ğŸ›¡ï¸ MANDATORY: Bug-Free Validation Before Handoff
**CRITICAL**: Every script MUST pass comprehensive validation before handoff.
**CRITICAL**: Every script MUST use sequential version numbering (v1, v2, v3...).

### 1. Validation Checklist (Complete ALL Before Handoff)
```python
# validation_checklist.py
import subprocess
import importlib
import sys
from pathlib import Path

def validate_all_scripts():
    """Comprehensive validation before handoff"""
    scripts_dir = Path("output/codes")
    all_passed = True
    
    print("ğŸ” SCRIPT VALIDATION CHECKLIST")
    print("=" * 50)
    
    for script in scripts_dir.glob("*.py"):
        print(f"\nğŸ“„ Validating: {script.name}")
        
        # 1. Syntax Check
        try:
            subprocess.run([sys.executable, "-m", "py_compile", str(script)], 
                         check=True, capture_output=True)
            print("  âœ… Syntax check passed")
        except subprocess.CalledProcessError:
            print("  âŒ Syntax error found!")
            all_passed = False
            continue
        
        # 2. Import Check
        try:
            spec = importlib.util.spec_from_file_location(script.stem, script)
            module = importlib.util.module_from_spec(spec)
            print("  âœ… Import check passed")
        except Exception as e:
            print(f"  âŒ Import error: {e}")
            all_passed = False
            continue
        
        # 3. Test Mode Check
        result = subprocess.run([sys.executable, str(script), "--test"], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("  âœ… Test mode passed")
        else:
            print(f"  âŒ Test mode failed: {result.stderr}")
            all_passed = False
        
        # 4. Help/Documentation Check
        result = subprocess.run([sys.executable, str(script), "--help"], 
                              capture_output=True, text=True)
        if "usage:" in result.stdout.lower():
            print("  âœ… Documentation check passed")
        else:
            print("  âŒ Missing help documentation")
            all_passed = False
    
    print("\n" + "=" * 50)
    if all_passed:
        print("âœ… ALL SCRIPTS VALIDATED SUCCESSFULLY!")
    else:
        print("âŒ VALIDATION FAILED - FIX ISSUES BEFORE HANDOFF")
    
    return all_passed

if __name__ == "__main__":
    validate_all_scripts()
```

### 2. Test Suite Template (MUST Include in Every Script)
```python
# Every script MUST include this test functionality
def run_tests():
    """Comprehensive test suite for validation"""
    print("ğŸ§ª Running validation tests...")
    
    # Test 1: Check imports
    try:
        import numpy as np
        import pandas as pd
        # Add all your imports here
        print("âœ… Import test passed")
    except ImportError as e:
        print(f"âŒ Import test failed: {e}")
        return False
    
    # Test 2: Check data paths
    required_paths = [
        Path("output/data"),
        Path("output/figures"),
        Path("output/models"),
        Path("output/logs")
    ]
    for path in required_paths:
        if not path.exists():
            path.mkdir(parents=True, exist_ok=True)
            print(f"ğŸ“ Created missing directory: {path}")
    print("âœ… Directory structure verified")
    
    # Test 3: Test core functions with minimal data
    try:
        # Create minimal test data
        test_data = pd.DataFrame({
            'x': np.random.rand(10),
            'y': np.random.rand(10)
        })
        
        # Test your main processing function
        result = process_data(test_data, test_mode=True)
        assert result is not None, "Process returned None"
        print("âœ… Core functionality test passed")
    except Exception as e:
        print(f"âŒ Core functionality test failed: {e}")
        return False
    
    # Test 4: Memory estimation
    estimated_memory = estimate_memory_usage(full_data_size=10000)
    print(f"ğŸ’¾ Estimated memory for full run: {estimated_memory:.2f} GB")
    
    # Test 5: Error handling
    try:
        # Test with invalid input
        process_data(None, test_mode=True)
    except ValueError as e:
        print("âœ… Error handling test passed")
    except Exception:
        print("âŒ Error handling test failed - should raise ValueError")
        return False
    
    print("\nâœ… ALL TESTS PASSED - Script is ready for handoff")
    return True

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Data Processing Script")
    parser.add_argument("--test", action="store_true", help="Run validation tests")
    parser.add_argument("--full", action="store_true", help="Run full analysis")
    args = parser.parse_args()
    
    if args.test:
        success = run_tests()
        sys.exit(0 if success else 1)
    elif args.full:
        main()
    else:
        parser.print_help()
```

### 3. Edge Case Testing Examples
```python
def test_edge_cases():
    """Test edge cases before handoff"""
    print("ğŸ” Testing edge cases...")
    
    # Empty data
    try:
        result = process_data(pd.DataFrame())
        print("âœ… Empty data handled correctly")
    except Exception as e:
        print(f"âŒ Failed on empty data: {e}")
    
    # Single row
    try:
        single_row = pd.DataFrame({'x': [1], 'y': [2]})
        result = process_data(single_row)
        print("âœ… Single row handled correctly")
    except Exception as e:
        print(f"âŒ Failed on single row: {e}")
    
    # Missing values
    try:
        data_with_nan = pd.DataFrame({
            'x': [1, np.nan, 3],
            'y': [4, 5, np.nan]
        })
        result = process_data(data_with_nan)
        print("âœ… Missing values handled correctly")
    except Exception as e:
        print(f"âŒ Failed on missing values: {e}")
    
    # Large values
    try:
        large_data = pd.DataFrame({
            'x': [1e10, 1e-10, 0],
            'y': [1e10, 1e-10, 0]
        })
        result = process_data(large_data)
        print("âœ… Extreme values handled correctly")
    except Exception as e:
        print(f"âŒ Failed on extreme values: {e}")
```

## ğŸ“‹ Script Execution Planning and Tracking

### 1. Dependency Analysis and Execution Order
**MANDATORY**: Before handoff, analyze ALL script dependencies to create a comprehensive execution plan.
```python
# generate_execution_plan.py
import json
from datetime import datetime
from pathlib import Path

def analyze_script_dependencies():
    """Analyze dependencies between scripts"""
    scripts = {
        "data_preprocessing.py": {
            "inputs": ["data/raw_data.csv"],
            "outputs": ["output/data/cleaned_data.csv"],
            "depends_on": [],
            "estimated_time": 300  # seconds
        },
        "feature_engineering.py": {
            "inputs": ["output/data/cleaned_data.csv"],
            "outputs": ["output/data/features.csv"],
            "depends_on": ["data_preprocessing.py"],
            "estimated_time": 600
        },
        "model_training.py": {
            "inputs": ["output/data/features.csv"],
            "outputs": ["output/models/trained_model.pkl"],
            "depends_on": ["feature_engineering.py"],
            "estimated_time": 3600
        },
        "generate_results.py": {
            "inputs": ["output/models/trained_model.pkl", "output/data/features.csv"],
            "outputs": ["output/figures/*.png", "output/data/results_*.csv"],
            "depends_on": ["model_training.py"],
            "estimated_time": 900
        }
    }
    
    # Generate execution order
    execution_order = []
    processed = set()
    
    def add_script(script_name):
        if script_name in processed:
            return
        # Process dependencies first
        for dep in scripts[script_name]["depends_on"]:
            add_script(dep)
        execution_order.append(script_name)
        processed.add(script_name)
    
    for script in scripts:
        add_script(script)
    
    return scripts, execution_order

def generate_execution_plan():
    """Generate comprehensive execution plan"""
    scripts, execution_order = analyze_script_dependencies()
    
    # Create execution plan
    plan = {
        "generated_at": datetime.now().isoformat(),
        "total_estimated_time": sum(s["estimated_time"] for s in scripts.values()),
        "execution_order": execution_order,
        "scripts": scripts,
        "parallel_groups": identify_parallel_groups(scripts, execution_order)
    }
    
    # Save plan
    with open("output/execution_plan.json", "w") as f:
        json.dump(plan, f, indent=2)
    
    # Generate visual representation
    print_execution_diagram(plan)
    
    return plan

def identify_parallel_groups(scripts, execution_order):
    """Identify which scripts can run in parallel"""
    groups = []
    current_group = []
    processed_outputs = set()
    
    for script in execution_order:
        # Check if dependencies are satisfied
        deps_satisfied = all(
            dep in processed_outputs or dep == script
            for dep in scripts[script]["depends_on"]
        )
        
        if deps_satisfied and current_group and can_run_parallel(script, current_group, scripts):
            current_group.append(script)
        else:
            if current_group:
                groups.append(current_group)
                # Add outputs to processed
                for s in current_group:
                    processed_outputs.add(s)
            current_group = [script]
    
    if current_group:
        groups.append(current_group)
    
    return groups

def print_execution_diagram(plan):
    """Print visual execution diagram"""
    print("\nğŸ“Š EXECUTION FLOW DIAGRAM")
    print("=" * 60)
    
    total_time = 0
    for i, group in enumerate(plan["parallel_groups"]):
        if len(group) == 1:
            script = group[0]
            time = plan["scripts"][script]["estimated_time"]
            print(f"\nStep {i+1}: {script}")
            print(f"         Time: {time//60} minutes")
            print(f"         Cumulative: {total_time//60} â†’ {(total_time+time)//60} minutes")
            total_time += time
        else:
            print(f"\nStep {i+1}: PARALLEL EXECUTION")
            max_time = 0
            for script in group:
                time = plan["scripts"][script]["estimated_time"]
                max_time = max(max_time, time)
                print(f"         - {script} ({time//60} min)")
            print(f"         Cumulative: {total_time//60} â†’ {(total_time+max_time)//60} minutes")
            total_time += max_time
    
    print(f"\nâ±ï¸  TOTAL ESTIMATED TIME: {total_time//60} minutes ({total_time//3600:.1f} hours)")
    print("=" * 60)
```

### 2. Progress Tracking System
```python
# execution_tracker.py
import json
from datetime import datetime
from pathlib import Path

class ExecutionTracker:
    def __init__(self, status_file="output/execution_status.json"):
        self.status_file = Path(status_file)
        self.load_or_create_status()
    
    def load_or_create_status(self):
        """Load existing status or create new"""
        if self.status_file.exists():
            with open(self.status_file) as f:
                self.status = json.load(f)
        else:
            # Load execution plan
            with open("output/execution_plan.json") as f:
                plan = json.load(f)
            
            # Initialize status
            self.status = {
                "started_at": None,
                "last_updated": None,
                "overall_status": "not_started",
                "scripts": {}
            }
            
            for script in plan["execution_order"]:
                self.status["scripts"][script] = {
                    "status": "not_started",
                    "started_at": None,
                    "completed_at": None,
                    "duration": None,
                    "outputs_generated": [],
                    "error": None
                }
            
            self.save_status()
    
    def update_script_status(self, script_name, status, error=None):
        """Update status for a specific script"""
        script_status = self.status["scripts"][script_name]
        script_status["status"] = status
        
        if status == "running":
            script_status["started_at"] = datetime.now().isoformat()
            if self.status["started_at"] is None:
                self.status["started_at"] = datetime.now().isoformat()
        elif status == "completed":
            script_status["completed_at"] = datetime.now().isoformat()
            if script_status["started_at"]:
                start = datetime.fromisoformat(script_status["started_at"])
                duration = (datetime.now() - start).total_seconds()
                script_status["duration"] = duration
            # Check outputs
            script_status["outputs_generated"] = self.check_outputs(script_name)
        elif status == "failed":
            script_status["error"] = error
        
        # Update overall status
        self.update_overall_status()
        self.status["last_updated"] = datetime.now().isoformat()
        self.save_status()
    
    def check_outputs(self, script_name):
        """Check which outputs were generated"""
        # Load execution plan to get expected outputs
        with open("output/execution_plan.json") as f:
            plan = json.load(f)
        
        outputs = []
        for output_pattern in plan["scripts"][script_name]["outputs"]:
            if "*" in output_pattern:
                # Handle glob patterns
                base_dir = Path(output_pattern).parent
                pattern = Path(output_pattern).name
                if base_dir.exists():
                    found = list(base_dir.glob(pattern))
                    outputs.extend(str(f) for f in found)
            else:
                # Check specific file
                if Path(output_pattern).exists():
                    outputs.append(output_pattern)
        
        return outputs
    
    def update_overall_status(self):
        """Update overall execution status"""
        statuses = [s["status"] for s in self.status["scripts"].values()]
        
        if all(s == "completed" for s in statuses):
            self.status["overall_status"] = "completed"
        elif any(s == "failed" for s in statuses):
            self.status["overall_status"] = "failed"
        elif any(s == "running" for s in statuses):
            self.status["overall_status"] = "running"
        elif any(s == "completed" for s in statuses):
            self.status["overall_status"] = "partial"
        else:
            self.status["overall_status"] = "not_started"
    
    def save_status(self):
        """Save status to file"""
        with open(self.status_file, "w") as f:
            json.dump(self.status, f, indent=2)
    
    def print_status(self):
        """Print current execution status"""
        print("\nğŸ“Š EXECUTION STATUS DASHBOARD")
        print("=" * 60)
        print(f"Overall Status: {self.status['overall_status'].upper()}")
        print(f"Last Updated: {self.status['last_updated'] or 'Never'}")
        print("\nScript Status:")
        
        for script, status in self.status["scripts"].items():
            icon = {
                "not_started": "â¸ï¸",
                "running": "ğŸ”„",
                "completed": "âœ…",
                "failed": "âŒ"
            }[status["status"]]
            
            print(f"\n{icon} {script}")
            print(f"   Status: {status['status']}")
            if status["duration"]:
                print(f"   Duration: {status['duration']//60:.1f} minutes")
            if status["outputs_generated"]:
                print(f"   Outputs: {len(status['outputs_generated'])} files generated")
            if status["error"]:
                print(f"   Error: {status['error']}")
        
        print("=" * 60)
```

### 3. Step-by-Step Execution Instructions Generator
```python
# generate_instructions.py
import json
from pathlib import Path

def generate_execution_instructions():
    """Generate detailed step-by-step instructions"""
    with open("output/execution_plan.json") as f:
        plan = json.load(f)
    
    instructions = []
    instructions.append("ğŸ“Š CODE EXECUTION INSTRUCTIONS")
    instructions.append("=" * 60)
    instructions.append("\n## Prerequisites Check")
    instructions.append("Before starting, ensure you have:")
    instructions.append("- [ ] Python environment activated: `source ~/.venv/ml_31123121/bin/activate`")
    instructions.append("- [ ] All required packages installed (check requirements.txt)")
    instructions.append("- [ ] At least X GB of free disk space")
    instructions.append("- [ ] At least Y GB of RAM available")
    
    instructions.append("\n## Execution Steps")
    
    cumulative_time = 0
    for i, script in enumerate(plan["execution_order"], 1):
        script_info = plan["scripts"][script]
        time_est = script_info["estimated_time"]
        
        instructions.append(f"\n### Step {i}: {script}")
        instructions.append(f"**Estimated time**: {time_est//60} minutes")
        instructions.append(f"**Cumulative time**: {cumulative_time//60} â†’ {(cumulative_time+time_est)//60} minutes")
        
        # Command
        instructions.append("\n**Command**:")
        instructions.append("```bash")
        instructions.append(f"cd output/codes")
        instructions.append(f"~/.venv/ml_31123121/bin/python {script} --full")
        instructions.append("```")
        
        # Update tracker
        instructions.append("\n**Update tracker** (before starting):")
        instructions.append("```bash")
        instructions.append(f'python execution_tracker.py --update {script} --status running')
        instructions.append("```")
        
        # Inputs
        if script_info["inputs"]:
            instructions.append("\n**Required inputs**:")
            for inp in script_info["inputs"]:
                instructions.append(f"- {inp}")
        
        # Expected outputs
        instructions.append("\n**Expected outputs**:")
        for out in script_info["outputs"]:
            instructions.append(f"- {out}")
        
        # Verification
        instructions.append("\n**Verify completion**:")
        instructions.append("```bash")
        instructions.append(f"# Check if outputs exist")
        for out in script_info["outputs"]:
            if "*" not in out:
                instructions.append(f"ls -la {out}")
        instructions.append("```")
        
        # Update tracker after completion
        instructions.append("\n**Update tracker** (after completion):")
        instructions.append("```bash")
        instructions.append(f'python execution_tracker.py --update {script} --status completed')
        instructions.append("# Or if failed:")
        instructions.append(f'python execution_tracker.py --update {script} --status failed --error "error message"')
        instructions.append("```")
        
        # Checkpoint
        if time_est > 1800:  # More than 30 minutes
            instructions.append("\n**Note**: This is a long-running script. It will save checkpoints to `output/checkpoints/`")
            instructions.append("If interrupted, you can resume from the last checkpoint.")
        
        cumulative_time += time_est
    
    instructions.append(f"\n## Total Estimated Time: {cumulative_time//3600:.1f} hours")
    
    # Troubleshooting section
    instructions.append("\n## Troubleshooting Guide")
    instructions.append("\n### Common Issues and Solutions:")
    instructions.append("\n1. **Memory Error**")
    instructions.append("   - Solution: Reduce batch size in config")
    instructions.append("   - Edit `output/codes/config.py` and set `BATCH_SIZE = 32`")
    
    instructions.append("\n2. **Import Error**")
    instructions.append("   - Solution: Install missing package")
    instructions.append("   - `pip install package_name`")
    
    instructions.append("\n3. **File Not Found**")
    instructions.append("   - Solution: Check previous script completed successfully")
    instructions.append("   - Run `python execution_tracker.py --status` to check")
    
    instructions.append("\n4. **GPU Not Available**")
    instructions.append("   - Solution: Scripts will automatically fall back to CPU")
    instructions.append("   - Note: This will increase runtime significantly")
    
    instructions.append("\n## Progress Monitoring")
    instructions.append("Check overall progress at any time:")
    instructions.append("```bash")
    instructions.append("python execution_tracker.py --status")
    instructions.append("```")
    
    instructions.append("\n## When Complete")
    instructions.append("After all scripts have completed successfully:")
    instructions.append("1. Run final verification: `python verify_outputs.py`")
    instructions.append("2. Check execution status: `python execution_tracker.py --status`")
    instructions.append("3. Reply with 'execution complete' to continue")
    
    # Save instructions
    with open("output/EXECUTION_INSTRUCTIONS.md", "w") as f:
        f.write("\n".join(instructions))
    
    print("\n".join(instructions))

if __name__ == "__main__":
    generate_execution_instructions()
```

### 4. EXECUTION_INSTRUCTIONS.md Template
This file is automatically generated and contains:
```markdown
# ğŸ“Š CODE EXECUTION INSTRUCTIONS
Generated on: [DATE]
Total Estimated Time: X.X hours

## Prerequisites Check
Before starting, ensure you have:
- [ ] Python environment activated: `source ~/.venv/ml_31123121/bin/activate`
- [ ] All required packages installed (check requirements.txt)
- [ ] At least X GB of free disk space
- [ ] At least Y GB of RAM available
- [ ] Validated all scripts: `python validation_checklist.py`
- [ ] Generated execution plan: `python generate_execution_plan.py`

## Script Execution Order
Based on dependency analysis, scripts must be run in this order:

### Step 1: data_preprocessing_v1.py
**Estimated time**: 5 minutes
**Cumulative time**: 0 â†’ 5 minutes

**Command**:
```bash
cd output/codes
~/.venv/ml_31123121/bin/python data_preprocessing_v1.py --full
# Output files will be saved with version numbers (e.g., cleaned_data_v1.csv)
```

**Update tracker** (before starting):
```bash
python execution_tracker.py --update data_preprocessing_v1.py --status running
```

**Required inputs**:
- data/raw_data.csv

**Expected outputs**:
- output/data/cleaned_data_v1.csv
- output/logs/preprocessing_v1.log

**Verify completion**:
```bash
# Check if outputs exist (with version)
ls -la output/data/cleaned_data_v1.csv
# Check log for errors
tail -n 20 output/logs/preprocessing_v1.log
```

**Update tracker** (after completion):
```bash
python execution_tracker.py --update data_preprocessing_v1.py --status completed
# Or if failed:
python execution_tracker.py --update data_preprocessing_v1.py --status failed --error "error message"
```

[Additional steps follow same format...]

## Progress Monitoring
Check overall progress at any time:
```bash
python execution_tracker.py --show
```

## Troubleshooting Guide
### Common Issues and Solutions:

1. **Memory Error**
   - Solution: Reduce batch size in config
   - Edit `output/codes/config.py` and set `BATCH_SIZE = 32`

2. **Import Error**
   - Solution: Install missing package
   - `pip install package_name`

3. **File Not Found**
   - Solution: Check previous script completed successfully
   - Run `python execution_tracker.py --show` to check

## When Complete
After all scripts have completed successfully:
1. Run final verification: `python verify_outputs.py`
2. Check execution status: `python execution_tracker.py --show`
3. Reply with 'execution complete' to continue
```

## Immediate Handoff Triggers
**STOP and handoff immediately if:**
- You receive "Command timed out" error
- Script contains model.fit(), trainer.train(), or similar
- Script has nested loops with >1000 total iterations
- Memory usage estimate >4GB
- ANY machine learning training detected
- Bash command fails with 2-minute timeout
- Script processes >10,000 data points
- Web scraping >100 pages

### Concrete Examples Requiring Handoff:
1. **Neural Network Training**
   ```python
   model.fit(X_train, y_train, epochs=10)  # Even 1 epoch â†’ HANDOFF
   ```

2. **Large Matrix Operations**
   ```python
   result = np.dot(matrix1, matrix2)  # If matrices >5000Ã—5000 â†’ HANDOFF
   ```

3. **Monte Carlo Simulations**
   ```python
   for i in range(10000):  # Large iterations â†’ HANDOFF
       simulate_scenario()
   ```

4. **Web Scraping**
   ```python
   for url in url_list:  # If >100 URLs â†’ HANDOFF
       scrape_page(url)
   ```

5. **Time Series Processing**
   ```python
   for window in sliding_windows:  # Long sequences â†’ HANDOFF
       analyze_window(window)
   ```

## When to Use This Prompt
- After code development and review approval
- When scripts involve:
  - Large dataset processing
  - Machine learning model training
  - Complex simulations or optimizations
  - Time-consuming computations (>5 minutes)
  - High memory/GPU requirements

## Execution Instructions Template

### 0. Pre-Execution Check
**BEFORE running any code:**
1. Check for ML training keywords: fit, train, epoch, optimizer
2. Estimate runtime: epochs Ã— batch_size Ã— model_complexity
3. If estimate >2 minutes â†’ IMMEDIATE HANDOFF
4. DO NOT attempt to reduce epochs or simplify - HANDOFF instead

**Runtime Estimation Formulas:**
- Neural Network: `time = epochs Ã— (dataset_size / batch_size) Ã— 0.1 seconds`
- Random Forest: `time = n_trees Ã— n_samples Ã— log(n_samples) Ã— 0.00001 seconds`
- Monte Carlo: `time = n_iterations Ã— operation_time`
- Matrix Operations: `time = nÂ³ Ã— 0.000001 seconds` (for nÃ—n matrices)
- Web Scraping: `time = n_pages Ã— (request_time + parse_time)` (~5-10 sec/page)

### 1. Pre-Execution Testing
Before handing off to user:
1. **Syntax Check**: Ensure all scripts are syntactically correct
2. **Import Test**: Verify all required packages can be imported
3. **Quick Test**: Run with minimal data (first 100 rows, 1 epoch, etc.)
4. **Error Handling**: Confirm proper error messages for common issues

```python
# Add this test mode to each script:
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--test", action="store_true", help="Run in test mode with minimal data")
    parser.add_argument("--full", action="store_true", help="Run full analysis")
    args = parser.parse_args()
    
    if args.test:
        print("Running in TEST mode with limited data...")
        # Use subset of data
        MAX_ROWS = 100
        MAX_ITERATIONS = 5
    else:
        print("Running FULL analysis...")
        MAX_ROWS = None
        MAX_ITERATIONS = 1000
```

### 2. Execution Instructions Format

```markdown
ğŸ“Š CODE EXECUTION REQUIRED
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

## Scripts to Execute

1. **Data Preprocessing** (Estimated time: X minutes)
   ```bash
   cd output/codes
   ~/.venv/ml_31123121/bin/python data_preprocessing.py --full
   ```
   - Input: data/raw_data.csv
   - Output: output/data/cleaned_data.csv
   - Purpose: Clean and prepare data for analysis

2. **Model Training** (Estimated time: X hours)
   ```bash
   ~/.venv/ml_31123121/bin/python model_training.py --full
   ```
   - Input: output/data/cleaned_data.csv
   - Output: output/models/trained_model.pkl
   - Purpose: Train machine learning model
   - Note: Requires GPU for optimal performance

3. **Results Generation** (Estimated time: X minutes)
   ```bash
   ~/.venv/ml_31123121/bin/python generate_results.py --full
   ```
   - Input: output/models/trained_model.pkl
   - Output: 
     - output/figures/*.png (visualizations)
     - output/data/results_*.csv (numerical results)
   - Purpose: Generate all figures and tables

## Execution Order
Scripts MUST be run in the order listed above as outputs from earlier scripts are inputs to later ones.

## Expected Outputs
After successful execution, you should have:
- [ ] Cleaned dataset in output/data/
- [ ] Trained model file(s) in output/models/
- [ ] All figures in output/figures/ (PNG format, 300 dpi)
- [ ] Result tables in output/data/ (CSV format)

## Troubleshooting

### Common Issues:
1. **Memory Error**: Reduce batch size or data subset
2. **Package Not Found**: Install with `pip install <package>`
3. **File Not Found**: Check paths are relative to project root
4. **GPU Not Available**: Scripts will fall back to CPU (slower)

### Progress Monitoring:
Each script will display progress bars and status messages.
Log files are saved to output/logs/

## When Complete
Please verify all expected outputs exist and then respond with:
- "execution complete" to continue with results analysis
- "execution failed: [error description]" if issues occurred
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### 3. Script Requirements

Each script should include:
1. **Progress indicators**
   ```python
   from tqdm import tqdm
   for i in tqdm(range(total_iterations), desc="Processing"):
       # processing code
   ```

2. **Logging**
   ```python
   import logging
   logging.basicConfig(
       filename='output/logs/script_name.log',
       level=logging.INFO,
       format='%(asctime)s - %(levelname)s - %(message)s'
   )
   ```

3. **Checkpointing** (for long-running processes)
   ```python
   import pickle
   
   def save_checkpoint(data, filename):
       with open(f'output/checkpoints/{filename}', 'wb') as f:
           pickle.dump(data, f)
   
   def load_checkpoint(filename):
       try:
           with open(f'output/checkpoints/{filename}', 'rb') as f:
               return pickle.load(f)
       except FileNotFoundError:
           return None
   ```

4. **Resource estimation**
   ```python
   def estimate_runtime(data_size):
       # Based on test runs
       seconds_per_1000_rows = 2.5
       estimated_seconds = (data_size / 1000) * seconds_per_1000_rows
       return f"{estimated_seconds/60:.1f} minutes"
   ```

### 4. Output Verification

After user completes execution, verify outputs:
```python
# verification_check.py
import os
from pathlib import Path

def verify_outputs():
    """Check all expected outputs exist"""
    missing = []
    
    # Check data files
    expected_data = [
        'output/data/cleaned_data.csv',
        'output/data/results_summary.csv',
        'output/data/statistical_tests.csv'
    ]
    
    # Check figures
    expected_figures = [
        'output/figures/figure1_distribution.png',
        'output/figures/figure2_correlation.png',
        'output/figures/figure3_model_performance.png'
    ]
    
    # Check models
    expected_models = [
        'output/models/trained_model.pkl',
        'output/models/model_metrics.json'
    ]
    
    all_expected = expected_data + expected_figures + expected_models
    
    for file_path in all_expected:
        if not Path(file_path).exists():
            missing.append(file_path)
    
    if missing:
        print("âŒ Missing outputs:")
        for f in missing:
            print(f"  - {f}")
        return False
    else:
        print("âœ… All expected outputs found!")
        return True

if __name__ == "__main__":
    verify_outputs()
```

### 5. Enhanced Handoff Message Template

When handing off to user:

```
ğŸ–¥ï¸ CODE EXECUTION HANDOFF
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

I've completed developing and thoroughly validating all Python scripts. Due to the computational requirements, you'll need to execute them on your system.

## ğŸ›¡ï¸ Validation Results
âœ… All scripts passed comprehensive validation:
   - Syntax validation: PASSED (X scripts)
   - Import verification: PASSED 
   - Test mode execution: PASSED (all functions tested)
   - Edge case handling: PASSED
   - Error handling: PASSED
   - Documentation: COMPLETE

## ğŸ“‹ Execution Plan Summary
Total Scripts: X
Execution Order: [List in order]
Dependencies: Mapped and verified
Total Estimated Time: X.X hours

## ğŸ“Š Progress Tracking
I've created the following tracking tools:
- `output/execution_plan.json` - Complete execution plan
- `output/execution_status.json` - Progress tracking file
- `output/EXECUTION_INSTRUCTIONS.md` - Step-by-step guide
- `output/codes/execution_tracker.py` - Status monitoring tool

## ğŸš€ Next Steps
1. Review the execution plan in EXECUTION_INSTRUCTIONS.md
2. Run validation check: `python output/codes/validation_checklist.py`
3. Follow the numbered steps in order
4. Update progress using the tracker after each script
5. Monitor overall status with: `python execution_tracker.py --status`

## âš ï¸ Important Notes
- All scripts have --test mode for quick validation
- Checkpointing enabled for scripts >30 minutes
- Troubleshooting guide included in instructions
- Scripts will auto-create required directories

Please proceed with execution and update me on progress. I'll be ready to analyze the results once all scripts complete successfully.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

## ğŸ”§ Utility Scripts for Validation and Tracking

### Create These Helper Scripts Before Handoff:

#### 1. Master Validation Script
Create `output/codes/validation_checklist.py`:
```python
#!/usr/bin/env python3
"""Master validation script to run before handoff"""
import subprocess
import sys
from pathlib import Path
import json
import importlib.util

def validate_all_scripts():
    """Run comprehensive validation on all scripts"""
    scripts_dir = Path("output/codes")
    results = {"passed": [], "failed": [], "warnings": []}
    
    print("ğŸ” COMPREHENSIVE SCRIPT VALIDATION")
    print("=" * 60)
    
    # Get all Python scripts
    scripts = sorted([f for f in scripts_dir.glob("*.py") 
                     if f.name not in ["validation_checklist.py", "__init__.py"]])
    
    for script in scripts:
        print(f"\nğŸ“„ Validating: {script.name}")
        script_results = validate_script(script)
        
        if script_results["status"] == "passed":
            results["passed"].append(script.name)
            print(f"  âœ… PASSED all validation checks")
        elif script_results["status"] == "warning":
            results["warnings"].append({
                "script": script.name,
                "issues": script_results["issues"]
            })
            print(f"  âš ï¸  PASSED with warnings")
        else:
            results["failed"].append({
                "script": script.name,
                "errors": script_results["errors"]
            })
            print(f"  âŒ FAILED validation")
    
    # Save results
    with open("output/validation_report.json", "w") as f:
        json.dump(results, f, indent=2)
    
    # Print summary
    print("\n" + "=" * 60)
    print("VALIDATION SUMMARY:")
    print(f"âœ… Passed: {len(results['passed'])} scripts")
    print(f"âš ï¸  Warnings: {len(results['warnings'])} scripts")
    print(f"âŒ Failed: {len(results['failed'])} scripts")
    
    if results["failed"]:
        print("\nâŒ VALIDATION FAILED - Fix errors before handoff!")
        return False
    else:
        print("\nâœ… VALIDATION PASSED - Ready for handoff!")
        return True

def validate_script(script_path):
    """Validate individual script"""
    results = {"status": "passed", "errors": [], "issues": []}
    
    # 1. Syntax check
    result = subprocess.run(
        [sys.executable, "-m", "py_compile", str(script_path)],
        capture_output=True, text=True
    )
    if result.returncode != 0:
        results["status"] = "failed"
        results["errors"].append(f"Syntax error: {result.stderr}")
        return results
    
    # 2. Check for test mode
    with open(script_path, 'r') as f:
        content = f.read()
        if "--test" not in content:
            results["status"] = "warning"
            results["issues"].append("No --test mode found")
    
    # 3. Check for error handling
    if "try:" not in content or "except" not in content:
        results["status"] = "warning"
        results["issues"].append("Limited error handling")
    
    # 4. Check for progress indicators
    if "tqdm" not in content and "print(" not in content:
        results["issues"].append("No progress indicators")
    
    # 5. Check for logging
    if "logging" not in content:
        results["issues"].append("No logging implemented")
    
    return results

if __name__ == "__main__":
    success = validate_all_scripts()
    sys.exit(0 if success else 1)
```

#### 2. Execution Plan Generator
Create `output/codes/generate_execution_plan.py`:
```python
#!/usr/bin/env python3
"""Generate execution plan with dependency analysis"""
import json
from datetime import datetime
from pathlib import Path
import re

def analyze_dependencies():
    """Analyze script dependencies based on input/output files"""
    scripts_dir = Path("output/codes")
    scripts_info = {}
    
    # Analyze each script
    for script in scripts_dir.glob("*.py"):
        if script.name in ["validation_checklist.py", "generate_execution_plan.py"]:
            continue
            
        with open(script, 'r') as f:
            content = f.read()
            
        # Extract input/output patterns
        inputs = extract_file_patterns(content, "input")
        outputs = extract_file_patterns(content, "output")
        
        # Estimate runtime
        runtime = estimate_runtime(content)
        
        scripts_info[script.name] = {
            "inputs": inputs,
            "outputs": outputs,
            "estimated_time": runtime,
            "depends_on": []
        }
    
    # Determine dependencies
    for script, info in scripts_info.items():
        for other_script, other_info in scripts_info.items():
            if script != other_script:
                # Check if this script's inputs are produced by other script
                for inp in info["inputs"]:
                    if inp in other_info["outputs"]:
                        info["depends_on"].append(other_script)
    
    return scripts_info

def extract_file_patterns(content, file_type):
    """Extract file paths from script content"""
    patterns = []
    
    if file_type == "input":
        # Look for read operations
        regex_patterns = [
            r'pd\.read_csv\(["\']([^"\']+)["\']',
            r'open\(["\']([^"\']+)["\'],\s*["\']r',
            r'np\.load\(["\']([^"\']+)["\']',
            r'pickle\.load.*["\']([^"\']+)["\']'
        ]
    else:
        # Look for write operations
        regex_patterns = [
            r'\.to_csv\(["\']([^"\']+)["\']',
            r'open\(["\']([^"\']+)["\'],\s*["\']w',
            r'plt\.savefig\(["\']([^"\']+)["\']',
            r'pickle\.dump.*["\']([^"\']+)["\']'
        ]
    
    for pattern in regex_patterns:
        matches = re.findall(pattern, content)
        patterns.extend(matches)
    
    return list(set(patterns))

def estimate_runtime(content):
    """Estimate runtime based on script content"""
    # Base estimate
    runtime = 60  # 1 minute base
    
    # Check for indicators of long runtime
    if "model.fit" in content or "train" in content:
        runtime += 3600  # Add 1 hour for training
    if "for" in content and "range(1000" in content:
        runtime += 600  # Add 10 minutes for large loops
    if "GridSearchCV" in content:
        runtime += 1800  # Add 30 minutes for grid search
    if "cross_val" in content:
        runtime += 900  # Add 15 minutes for cross-validation
    
    return runtime

def generate_execution_order(scripts_info):
    """Generate execution order based on dependencies"""
    execution_order = []
    processed = set()
    
    def add_script(script_name):
        if script_name in processed:
            return
        # Process dependencies first
        for dep in scripts_info[script_name]["depends_on"]:
            add_script(dep)
        execution_order.append(script_name)
        processed.add(script_name)
    
    # Process all scripts
    for script in scripts_info:
        add_script(script)
    
    return execution_order

def save_execution_plan(scripts_info, execution_order):
    """Save execution plan to JSON"""
    total_time = sum(info["estimated_time"] for info in scripts_info.values())
    
    plan = {
        "generated_at": datetime.now().isoformat(),
        "total_estimated_time_seconds": total_time,
        "total_estimated_time_hours": round(total_time / 3600, 1),
        "execution_order": execution_order,
        "scripts": scripts_info
    }
    
    with open("output/execution_plan.json", "w") as f:
        json.dump(plan, f, indent=2)
    
    print(f"âœ… Execution plan saved to output/execution_plan.json")
    print(f"ğŸ“Š Total estimated time: {plan['total_estimated_time_hours']} hours")
    print(f"ğŸ“ Execution order: {' â†’ '.join(execution_order)}")
    
    return plan

if __name__ == "__main__":
    print("ğŸ” Analyzing script dependencies...")
    scripts_info = analyze_dependencies()
    
    print("ğŸ“Š Generating execution order...")
    execution_order = generate_execution_order(scripts_info)
    
    print("ğŸ’¾ Saving execution plan...")
    plan = save_execution_plan(scripts_info, execution_order)
```

#### 3. Execution Tracker
Create `output/codes/execution_tracker.py`:
```python
#!/usr/bin/env python3
"""Track execution progress of scripts"""
import json
import argparse
from datetime import datetime
from pathlib import Path

class ExecutionTracker:
    def __init__(self):
        self.status_file = Path("output/execution_status.json")
        self.load_or_create_status()
    
    def load_or_create_status(self):
        """Load existing status or create new"""
        if self.status_file.exists():
            with open(self.status_file) as f:
                self.status = json.load(f)
        else:
            # Initialize from execution plan
            if Path("output/execution_plan.json").exists():
                with open("output/execution_plan.json") as f:
                    plan = json.load(f)
                
                self.status = {
                    "started_at": None,
                    "last_updated": None,
                    "overall_status": "not_started",
                    "scripts": {}
                }
                
                for script in plan["execution_order"]:
                    self.status["scripts"][script] = {
                        "status": "not_started",
                        "started_at": None,
                        "completed_at": None,
                        "duration_seconds": None,
                        "outputs_verified": False,
                        "error": None
                    }
            else:
                raise FileNotFoundError("execution_plan.json not found. Run generate_execution_plan.py first.")
        
        self.save_status()
    
    def update_script_status(self, script_name, status, error=None):
        """Update status for a specific script"""
        if script_name not in self.status["scripts"]:
            print(f"âŒ Script {script_name} not in execution plan")
            return
        
        script_info = self.status["scripts"][script_name]
        script_info["status"] = status
        
        if status == "running":
            script_info["started_at"] = datetime.now().isoformat()
            if self.status["started_at"] is None:
                self.status["started_at"] = datetime.now().isoformat()
        
        elif status == "completed":
            script_info["completed_at"] = datetime.now().isoformat()
            if script_info["started_at"]:
                start = datetime.fromisoformat(script_info["started_at"])
                duration = (datetime.now() - start).total_seconds()
                script_info["duration_seconds"] = duration
            script_info["outputs_verified"] = self.verify_outputs(script_name)
        
        elif status == "failed":
            script_info["error"] = error or "Unknown error"
        
        self.update_overall_status()
        self.status["last_updated"] = datetime.now().isoformat()
        self.save_status()
        
        print(f"âœ… Updated {script_name} status to: {status}")
    
    def verify_outputs(self, script_name):
        """Verify expected outputs exist"""
        # Load execution plan to check expected outputs
        with open("output/execution_plan.json") as f:
            plan = json.load(f)
        
        if script_name not in plan["scripts"]:
            return False
        
        expected_outputs = plan["scripts"][script_name]["outputs"]
        all_exist = True
        
        for output_path in expected_outputs:
            if not Path(output_path).exists():
                all_exist = False
                print(f"  âš ï¸  Missing output: {output_path}")
        
        return all_exist
    
    def update_overall_status(self):
        """Update overall execution status"""
        statuses = [s["status"] for s in self.status["scripts"].values()]
        
        if all(s == "completed" for s in statuses):
            self.status["overall_status"] = "completed"
        elif any(s == "failed" for s in statuses):
            self.status["overall_status"] = "failed"
        elif any(s == "running" for s in statuses):
            self.status["overall_status"] = "running"
        elif any(s == "completed" for s in statuses):
            self.status["overall_status"] = "partial"
        else:
            self.status["overall_status"] = "not_started"
    
    def save_status(self):
        """Save status to file"""
        with open(self.status_file, "w") as f:
            json.dump(self.status, f, indent=2)
    
    def print_status(self):
        """Print current execution status"""
        print("\nğŸ“Š EXECUTION STATUS DASHBOARD")
        print("=" * 60)
        print(f"Overall Status: {self.status['overall_status'].upper()}")
        print(f"Last Updated: {self.status.get('last_updated', 'Never')}")
        
        # Calculate progress
        total_scripts = len(self.status["scripts"])
        completed = sum(1 for s in self.status["scripts"].values() if s["status"] == "completed")
        progress = (completed / total_scripts * 100) if total_scripts > 0 else 0
        
        print(f"Progress: {completed}/{total_scripts} scripts ({progress:.1f}%)")
        print("\nScript Details:")
        
        for script, info in self.status["scripts"].items():
            icon = {
                "not_started": "â¸ï¸",
                "running": "ğŸ”„",
                "completed": "âœ…",
                "failed": "âŒ"
            }[info["status"]]
            
            print(f"\n{icon} {script}")
            print(f"   Status: {info['status']}")
            
            if info["duration_seconds"]:
                duration_min = info["duration_seconds"] / 60
                print(f"   Duration: {duration_min:.1f} minutes")
            
            if info["status"] == "completed":
                print(f"   Outputs verified: {'âœ…' if info['outputs_verified'] else 'âŒ'}")
            
            if info["error"]:
                print(f"   Error: {info['error']}")
        
        print("=" * 60)

def main():
    parser = argparse.ArgumentParser(description="Track script execution progress")
    parser.add_argument("--update", metavar="SCRIPT", help="Update script status")
    parser.add_argument("--status", choices=["running", "completed", "failed"], 
                       help="New status for the script")
    parser.add_argument("--error", help="Error message (for failed status)")
    parser.add_argument("--show", action="store_true", help="Show current status")
    
    args = parser.parse_args()
    
    tracker = ExecutionTracker()
    
    if args.update and args.status:
        tracker.update_script_status(args.update, args.status, args.error)
    elif args.show or (not args.update):
        tracker.print_status()
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
```

## Best Practices

1. **Always test first**: Run scripts in test mode before full execution
2. **Clear documentation**: Include docstrings and comments
3. **Error messages**: Provide helpful error messages with solutions
4. **Progress tracking**: Use progress bars for long operations
5. **Intermediate saves**: Save intermediate results for long computations
6. **Memory efficiency**: Process large datasets in chunks
7. **Reproducibility**: Set random seeds and document parameters
8. **Validation before handoff**: Run validation_checklist.py
9. **Dependency mapping**: Use generate_execution_plan.py
10. **Progress monitoring**: Provide execution_tracker.py to user

## Integration with Workflow

This step occurs between:
- Previous: Code Development & Review (scripts approved)
- Next: Results and Discussion (using execution outputs)

The user must successfully execute all scripts before proceeding to results analysis.

## âš ï¸ CRITICAL: Kill Switch for Version Creation

**DO NOT attempt to fix timeouts by simplifying code!**

If you encounter a timeout:
- âŒ DO NOT create v2/v3 with fewer epochs
- âŒ DO NOT reduce data size to avoid timeout
- âŒ DO NOT simplify algorithms to run faster
- âœ… IMMEDIATELY handoff to user
- âœ… Provide full version for user execution
- âœ… Include runtime estimates

**Remember**: A timeout is a signal to HANDOFF, not a problem to solve!

## ğŸ›¡ï¸ Enhanced Bug-Free Validation Examples from CLAUDE.md

### Comprehensive Testing Phase Details
1. **Syntax validation for all scripts**
   ```python
   python -m py_compile script.py
   ```
2. **Import verification with virtual environment**
   ```python
   # Test all imports work in the correct environment
   ~/.venv/ml_31123121/bin/python -c "import numpy, pandas, torch"
   ```
3. **Logic testing with minimal/synthetic data**
   - Always use --test mode with <100 data points
   - Test critical functions independently
4. **Edge case handling verification**
   - Empty data, single row, missing values, extreme values
5. **Error message clarity check**
   - Ensure errors provide actionable solutions

### Test Suite Requirements Details
- Every script MUST have `--test` mode that:
  - Runs with <100 data points
  - Validates all functions work correctly
  - Checks input/output file paths
  - Verifies dependencies are installed
  - Completes in <30 seconds

### Validation Checklist Details (MUST complete ALL)
- [ ] All scripts pass syntax check (`python -m py_compile script.py`)
- [ ] All imports verified in correct venv
- [ ] Test mode executes successfully
- [ ] Error handling tested with invalid inputs
- [ ] Output directories created automatically
- [ ] Progress indicators functioning
- [ ] Logging system operational
- [ ] Memory usage estimated and documented

### Script Execution Planning Details

#### Execution Order Analysis
- Identify script dependencies (output of A is input to B)
- Create execution DAG (Directed Acyclic Graph)
- Determine parallel vs sequential requirements
- Calculate cumulative time estimates

#### Step-by-Step Instructions Format
- Number each script in execution order
- Specify exact commands with full paths
- Include expected runtime for each
- Document required inputs/outputs
- Add verification steps between scripts

#### Progress Tracking System Requirements
- Create `execution_status.json` for tracking
- Update status: not_started â†’ running â†’ completed/failed
- Include timestamps and duration
- Enable pause/resume capability
- Track which outputs have been generated

### Enhanced Handoff Process Steps
1. **Develop and validate** all scripts thoroughly
2. **Run comprehensive tests** ensuring bug-free operation
3. **Generate execution plan** with dependencies mapped
4. **Create tracking system** for progress monitoring
5. **Provide detailed instructions** with troubleshooting
6. **User executes scripts** following the plan
7. **User updates progress** in tracking system
8. **Continue with results** using provided outputs

## Python Execution Command Examples

### Virtual Environment Usage
```bash
# Machine Learning Environment (default for most scripts)
~/.venv/ml_31123121/bin/python output/codes/analysis.py

# Web Scraping Environment (for scraping scripts)
~/.venv/webScraping/bin/python utilityScripts/scrape_papers.py
```

### Common Execution Patterns
```bash
# Run data analysis scripts
~/.venv/ml_31123121/bin/python output/codes/data_preprocessing.py
~/.venv/ml_31123121/bin/python output/codes/model_training.py --epochs 100
~/.venv/ml_31123121/bin/python output/codes/results_analysis.py

# Run utility scripts
~/.venv/ml_31123121/bin/python utilityScripts/extract_section_citations.py
~/.venv/ml_31123121/bin/python utilityScripts/detect_latest_sections.py --analyze-content

# Test mode execution (always test first!)
~/.venv/ml_31123121/bin/python output/codes/analysis.py --test
```

### Script Execution Order
Always follow the dependency order specified in `execution_plan.json`:
1. Data preprocessing scripts first
2. Model training/analysis scripts second
3. Visualization/results generation last

## Immediate Code Execution Triggers from CLAUDE.md

### When to Immediately Handoff to User
**STOP and handoff immediately if any of these conditions occur:**
- Bash command times out (2 minutes)
- ANY machine learning model training (even 1 epoch)
- Iterative algorithms with >100 iterations
- Script processes >10,000 data points
- Web scraping >100 pages
- Memory usage estimate >4GB
- Matrix operations with dimensions >5000Ã—5000
- Monte Carlo simulations with large iterations

### Concrete Timeout Examples from CLAUDE.md
1. **Neural Network Training**
   ```python
   model.fit(X_train, y_train, epochs=10)  # Even 1 epoch â†’ IMMEDIATE HANDOFF
   ```

2. **Large Matrix Operations**
   ```python
   result = np.dot(matrix1, matrix2)  # If matrices >5000Ã—5000 â†’ HANDOFF
   ```

3. **Monte Carlo Simulations**
   ```python
   for i in range(10000):  # Large iterations â†’ HANDOFF
       simulate_scenario()
   ```

4. **Web Scraping**
   ```python
   for url in url_list:  # If >100 URLs â†’ HANDOFF
       scrape_page(url)
   ```

5. **Time Series Processing**
   ```python
   for window in sliding_windows:  # Long sequences â†’ HANDOFF
       analyze_window(window)
   ```

### Runtime Estimation Formulas from CLAUDE.md
- Neural Network: `time = epochs Ã— (dataset_size / batch_size) Ã— 0.1 seconds`
- Random Forest: `time = n_trees Ã— n_samples Ã— log(n_samples) Ã— 0.00001 seconds`
- Monte Carlo: `time = n_iterations Ã— operation_time`
- Matrix Operations: `time = nÂ³ Ã— 0.000001 seconds` (for nÃ—n matrices)
- Web Scraping: `time = n_pages Ã— (request_time + parse_time)` (~5-10 sec/page)

### Pre-Execution Estimation Check
**BEFORE running any code:**
1. Check for ML training keywords: fit, train, epoch, optimizer
2. Estimate runtime using formulas above
3. If estimate >2 minutes â†’ IMMEDIATE HANDOFF
4. DO NOT attempt to reduce epochs or simplify - HANDOFF instead

### Kill Switch for Version Creation
**DO NOT attempt to fix timeouts by simplifying code!**
- âŒ DO NOT create v2/v3 with fewer epochs
- âŒ DO NOT reduce data size to avoid timeout
- âŒ DO NOT simplify algorithms to run faster
- âœ… IMMEDIATELY handoff to user
- âœ… Provide full version for user execution
- âœ… Include runtime estimates

**Remember**: A timeout is a signal to HANDOFF, not a problem to solve!