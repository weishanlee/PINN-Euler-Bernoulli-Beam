# Data Analysis and Visualization Guidelines

## Overview
This guide provides comprehensive instructions for analyzing data, creating visualizations, and performing statistical analyses for academic papers. It covers data cleansing, exploratory data analysis (EDA), and advanced analytical techniques.

## ðŸ“Š Core Analysis Workflow

### 1. Data Preparation Phase

#### Initial Assessment
- **Load data** from `data/` folder (CSV/Excel files)
- **Examine structure**: columns, data types, dimensions
- **Identify issues**: missing values, duplicates, inconsistencies
- **Document findings** for transparency

#### Data Cleaning Steps
1. **Missing Values**
   - Identify patterns of missingness
   - Choose appropriate strategy:
     - Deletion (if < 5% missing)
     - Imputation (mean/median/mode for numeric)
     - Forward/backward fill (for time series)
     - Advanced methods (KNN, MICE)

2. **Duplicate Removal**
   - Identify exact and fuzzy duplicates
   - Preserve meaningful variations
   - Document removal criteria

3. **Outlier Treatment**
   - Statistical methods (IQR, Z-score)
   - Domain-specific thresholds
   - Visualization-based detection
   - Decision: remove, transform, or retain with justification

4. **Data Standardization**
   - Consistent formats (dates, currencies)
   - Text normalization (case, spacing)
   - Unit conversions
   - Category consolidation

### 2. Exploratory Data Analysis (EDA)

#### Descriptive Statistics
Generate comprehensive summaries including:
- **Central tendency**: mean, median, mode
- **Dispersion**: std dev, variance, range
- **Shape**: skewness, kurtosis
- **Percentiles**: quartiles, deciles

#### Correlation Analysis
- **Pearson correlation** for linear relationships
- **Spearman correlation** for monotonic relationships
- **Correlation matrices** with heatmaps
- **Feature relationships** identification

### 3. Visualization Suite

#### Basic Visualizations
Create these standard plots with proper styling:

1. **Line Plot** - Temporal trends and sequences
2. **Scatter Plot** - Bivariate relationships
3. **Bar Chart** - Categorical comparisons
4. **Pie Chart** - Proportional data (use sparingly)
5. **Box Plot** - Distribution and outliers
6. **Histogram** - Frequency distributions
7. **Heatmap** - Correlation or intensity matrices
8. **Area Plot** - Cumulative values over time

#### Advanced Visualizations
For deeper insights, create:

9. **3D Scatter Plot** - Three-dimensional relationships
10. **Polar Plot** - Cyclical or directional data
11. **Hexbin Plot** - Dense scatter plot alternatives
12. **Venn Diagram** - Set intersections
13. **Sankey Diagram** - Flow relationships
14. **Chord Diagram** - Interconnections
15. **Sunburst Chart** - Hierarchical data
16. **Stream Graph** - Stacked area variations

#### Specialized EDA Visualizations
17. **Multi-level Donut Chart** - Nested categorical data
18. **Sociogram** - Network relationships
19. **Proportional Area Chart** - Size comparisons
20. **Phase Diagram** - State transitions
21. **3D Radar Diagram** - Multivariate comparisons

### 4. Advanced Statistical Analyses

#### Dimensionality Reduction
1. **PCA (Principal Component Analysis)**
   - Variance explained
   - Scree plots
   - Loading plots
   - Biplot visualization

2. **t-SNE (t-distributed Stochastic Neighbor Embedding)**
   - Non-linear dimensionality reduction
   - Cluster visualization
   - Perplexity tuning

#### Discriminant Analysis
3. **PLS-DA (Partial Least Squares Discriminant Analysis)**
   - Supervised classification
   - Variable importance plots
   - Score plots

4. **OPLS-DA (Orthogonal PLS-DA)**
   - Improved interpretability
   - Predictive vs orthogonal components

#### Statistical Testing
5. **ANOVA (Analysis of Variance)**
   - One-way, two-way, or multiway
   - Post-hoc tests (Tukey, Bonferroni)
   - Effect size calculations

### 5. Implementation Guidelines

#### Python Code Structure
```python
# Example structure for each analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Set style and color palette
plt.style.use('seaborn-v0_8-darkgrid')
colors = ['#9FCDC9', '#56AEDE', '#EE7A5F', '#FDD39F', '#B6C4BA']  # From colorSet

# Load and clean data
def load_and_clean_data(filepath):
    """Load data and perform initial cleaning"""
    df = pd.read_csv(filepath)
    # Cleaning steps...
    return df

# Create visualizations
def create_visualization(df, viz_type):
    """Generate specified visualization"""
    fig, ax = plt.subplots(figsize=(10, 6))
    # Visualization code...
    plt.savefig(f'output/figures/{viz_type}.png', dpi=300, bbox_inches='tight')
    return fig

# Perform analysis
def perform_analysis(df, analysis_type):
    """Execute statistical analysis"""
    # Analysis code...
    return results
```

#### Output Organization
- **Python scripts**: Save to `output/codes/`
- **Figures**: Save to `output/figures/` as PNG (300 dpi)
- **Data tables**: Save to `output/data/` as CSV
- **Analysis results**: Document in code comments

### 6. LaTeX Integration

#### Figure Documentation Template
For each figure, write a 150+ word explanation:

```latex
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/analysis_plot.png}
    \caption{Comprehensive analysis of [describe what the figure shows]. 
    The visualization reveals [key finding 1], which indicates [interpretation]. 
    Additionally, [key finding 2] suggests [implication]. The [specific feature] 
    demonstrates [pattern/trend], supporting the hypothesis that [conclusion]. 
    This analysis is crucial for understanding [broader context].}
    \label{fig:analysis_plot}
\end{figure}
```

#### Table Documentation Template
```latex
\begin{table}[H]
    \centering
    \caption{Statistical summary of key variables}
    \label{tab:summary_stats}
    \begin{tabular}{lrrrr}
    \hline
    \textbf{Variable} & \textbf{Mean} & \textbf{Std Dev} & \textbf{Min} & \textbf{Max} \\
    \hline
    Variable 1 & 45.2 & 12.3 & 10.0 & 89.5 \\
    Variable 2 & 73.8 & 8.7 & 55.2 & 95.3 \\
    \hline
    \end{tabular}
\end{table}
```

### 7. Quality Checklist

Before finalizing:
- [ ] All data cleaning steps documented
- [ ] Missing data handling justified
- [ ] Outliers addressed appropriately
- [ ] All required visualizations created
- [ ] Color schemes from `prompts_colorSet` applied
- [ ] Statistical tests assumptions verified
- [ ] Results interpreted in context
- [ ] Code is reproducible and well-commented
- [ ] All outputs saved to correct folders
- [ ] LaTeX descriptions written for all figures/tables

### 8. MANDATORY Double-Check Protocol

#### Data Integrity Verification
```python
# data_verification.py
import pandas as pd
import hashlib
from pathlib import Path

def verify_data_integrity():
    """Verify all data files are valid and documented"""
    errors = []
    data_dir = Path("output/data")
    
    for csv_file in data_dir.glob("*.csv"):
        # Check file can be read
        try:
            df = pd.read_csv(csv_file)
            if df.empty:
                errors.append(f"{csv_file.name}: Empty dataset")
        except Exception as e:
            errors.append(f"{csv_file.name}: Cannot read - {e}")
        
        # Check documentation exists
        doc_file = csv_file.with_suffix('.txt')
        if not doc_file.exists():
            errors.append(f"{csv_file.name}: Missing documentation")
    
    return errors

# Run verification
errors = verify_data_integrity()
if errors:
    print("âŒ Data verification failed:")
    for error in errors:
        print(f"  - {error}")
    exit(1)
```

#### Visualization Double-Check
```python
# figure_verification.py
import matplotlib.pyplot as plt
from pathlib import Path

def verify_all_figures():
    """Ensure all figures exist and use correct colors"""
    fig_dir = Path("output/figures")
    required_figs = set()  # Populated from analysis scripts
    
    # Check all required figures exist
    for fig in required_figs:
        if not (fig_dir / fig).exists():
            print(f"âŒ Missing figure: {fig}")
            return False
    
    # Verify color scheme usage
    approved_colors = ['#9FCDC9', '#56AEDE', '#EE7A5F', '#FDD39F', '#B6C4BA']
    # Add color validation logic
    
    return True
```

#### Statistical Validity Check
- Verify normality before t-tests
- Check independence for ANOVA
- Validate sample sizes for all tests
- Document all assumption violations

#### Final Analysis Checklist
**DO NOT proceed if any check fails:**
- [ ] All data files verified and documented
- [ ] Visualizations match required list
- [ ] Statistical assumptions validated
- [ ] Reproducibility tested (re-run all scripts)
- [ ] No hardcoded paths in scripts
- [ ] Random seeds set for reproducibility
- [ ] All outputs have consistent formatting

### 8. Special Considerations

#### For Competition Papers
- Focus on insights directly relevant to problem
- Emphasize practical implications
- Create clear, impactful visualizations
- Ensure reproducibility

#### For Journal Papers
- Follow discipline-specific conventions
- Include detailed statistical methodology
- Provide comprehensive supplementary materials
- Address potential reviewer concerns

### 9. Common Pitfalls to Avoid
- Over-cleaning data (removing real variability)
- Cherry-picking visualizations
- Ignoring assumptions of statistical tests
- Creating cluttered or unclear plots
- Forgetting to set random seeds for reproducibility
- Using inappropriate color schemes for colorblind readers