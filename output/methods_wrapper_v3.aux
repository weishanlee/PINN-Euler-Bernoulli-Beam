\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{almajid2021physics}
\citation{kapoor2023physics}
\citation{hu2024hutchinson}
\citation{liu2024machine}
\citation{wang2021understanding}
\citation{krishnapriyan2021characterizing}
\citation{han1999dynamics}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methodology}{1}{section.2}\protected@file@percent }
\newlabel{sec:method}{{2}{1}{Methodology}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Theoretical Framework: Addressing the Ultra-Precision Challenge}{1}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Assumptions and Justification}{1}{subsection.2.2}\protected@file@percent }
\citation{raissi2019physics}
\citation{wong2022learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Notations}{2}{subsection.2.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Symbol Descriptions}}{2}{table.1}\protected@file@percent }
\newlabel{tab:symbols}{{1}{2}{Symbol Descriptions}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Hybrid Fourier-Neural Architecture: The Breakthrough Design}{2}{subsection.2.4}\protected@file@percent }
\newlabel{eq:hybrid_solution}{{1}{2}{Hybrid Fourier-Neural Architecture: The Breakthrough Design}{equation.2.1}{}}
\citation{mcclenny2023self}
\citation{wang2021understanding}
\citation{mcclenny2020self}
\citation{penwarden2023unified}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Physics-Informed Loss Function with Adaptive Weighting}{4}{subsection.2.5}\protected@file@percent }
\newlabel{eq:euler_bernoulli}{{7}{4}{Physics-Informed Loss Function with Adaptive Weighting}{equation.2.7}{}}
\citation{kingma2014adam}
\citation{liu1989limited}
\citation{jagtap2020conservative}
\citation{psaros2023uncertainty}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Two-Phase Optimization Strategy: Breaking the Precision Barrier}{5}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}GPU-Efficient Implementation}{5}{subsection.2.7}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Ultra-Precision PINN Training Algorithm}}{6}{algorithm.1}\protected@file@percent }
\newlabel{alg:training}{{1}{6}{Two-Phase Optimization Strategy: Breaking the Precision Barrier}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Sensitivity Analysis and Harmonic Discovery}{6}{subsection.2.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Hybrid Fourier-PINN architecture for the Euler-Bernoulli beam equation showing both forward pass (solid arrows) and backward propagation (dashed arrows). The architecture combines a truncated Fourier series expansion (10 harmonics) with a 7-layer deep neural network (2→128→128→64→32→16→8→1 neurons). The Fourier coefficients $A_n$ and $B_n$ are learnable parameters trained through backpropagation, not outputs from the neural network. Boundary conditions are enforced through multiplication with $\qopname  \relax o{sin}(\pi x/L)$. The two-phase optimization strategy achieves L2 error of $1.94 \times 10^{-7}$.}}{7}{figure.1}\protected@file@percent }
\newlabel{fig:architecture}{{1}{7}{Hybrid Fourier-PINN architecture for the Euler-Bernoulli beam equation showing both forward pass (solid arrows) and backward propagation (dashed arrows). The architecture combines a truncated Fourier series expansion (10 harmonics) with a 7-layer deep neural network (2→128→128→64→32→16→8→1 neurons). The Fourier coefficients $A_n$ and $B_n$ are learnable parameters trained through backpropagation, not outputs from the neural network. Boundary conditions are enforced through multiplication with $\sin (\pi x/L)$. The two-phase optimization strategy achieves L2 error of $1.94 \times 10^{-7}$}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training workflow and optimization strategy for the ultra-precision PINN. The methodology employs a two-phase approach: initial Adam optimization for rapid convergence followed by L-BFGS refinement for ultra-high precision. Dynamic memory management and adaptive weight balancing ensure stable training throughout both phases.}}{8}{figure.2}\protected@file@percent }
\newlabel{fig:workflow}{{2}{8}{Training workflow and optimization strategy for the ultra-precision PINN. The methodology employs a two-phase approach: initial Adam optimization for rapid convergence followed by L-BFGS refinement for ultra-high precision. Dynamic memory management and adaptive weight balancing ensure stable training throughout both phases}{figure.2}{}}
\bibstyle{ieee}
\bibdata{methods_refs}
\bibcite{almajid2021physics}{1}
\bibcite{kapoor2023physics}{2}
\bibcite{hu2024hutchinson}{3}
\bibcite{liu2024machine}{4}
\bibcite{wang2021understanding}{5}
\bibcite{krishnapriyan2021characterizing}{6}
\bibcite{han1999dynamics}{7}
\bibcite{raissi2019physics}{8}
\bibcite{wong2022learning}{9}
\bibcite{mcclenny2023self}{10}
\bibcite{mcclenny2020self}{11}
\bibcite{penwarden2023unified}{12}
\bibcite{kingma2014adam}{13}
\bibcite{liu1989limited}{14}
\bibcite{jagtap2020conservative}{15}
\bibcite{psaros2023uncertainty}{16}
\gdef \@abspage@last{10}
