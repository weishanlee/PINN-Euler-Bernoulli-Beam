\documentclass[Journal,letterpaper]{ascelike-new}
\WarningFilter{caption}{Unknown document class}
%% Please choose the appropriate document class option:
% "Journal" produces double-spaced manuscripts for ASCE journals.
% "NewProceedings" produces single-spaced manuscripts for ASCE conference proceedings.
% "Proceedings" produces older-style single-spaced manuscripts for ASCE conference proceedings. 
%
%% For more details and options, please see the notes in the ascelike-new.cls file.

% Some useful packages...
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{a4paper, left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}
\usepackage[style=base,figurename=Fig.,labelfont=bf,labelsep=period]{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{listings}
%\usepackage{amsfonts}
%\usepackage{amssymb}
%\usepackage{amsbsy}
\usepackage{newtxtext,newtxmath}
\usepackage[colorlinks=true,citecolor=red,linkcolor=black]{hyperref}
%
% Please add the first author's last name here for the footer:
\NameTag{AuthorOneLastName, \today}
% Note that this is not displayed if the NoPageNumbers option is used
% in the documentclass declaration.
%
\begin{document}

% You will need to make the title all-caps
\title{Hybrid CNN-LSTM Physics-Informed Neural Network for the Euler-Bernoulli Beam Equation}

\author[1]{Author One}
\author[2]{Author Two}
\author[3]{Author Three}

\affil[1]{First affiliation address, with corresponding author email. Email: author.one@email.com}
\affil[2]{Second affiliation address}
\affil[2]{Third affiliation address}

\maketitle

% Please include an abstract:
\begin{abstract}
This paper presents a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) architecture applied in a Physics-Informed Neural Network (PINN) framework to solve the Euler-Bernoulli beam equation. The CNN handles spatial feature extraction, while the LSTM captures temporal dependencies. We incorporate attention mechanisms for enhanced spatial and temporal feature refinement and propose a comprehensive loss function that penalizes deviations from initial/boundary conditions and the governing partial differential equation (PDE). By leveraging PyTorch's automatic differentiation and a tailored loss definition, the hybrid CNN-LSTM PINN learns beam dynamics accurately with reduced reliance on large datasets.
\end{abstract}

\section{Introduction}
Physics-Informed Neural Networks (PINNs) have emerged as a promising approach to solving PDE-governed problems by embedding physical laws in the training process \cite{raissi2019physics,karniadakis2021physics}. In this work, we tackle the Euler-Bernoulli beam equation, a fourth-order PDE governing beam deflection under bending and dynamic loading \cite{timoshenko1959theory}. While traditional PINNs often rely on fully-connected networks to map $(x,t)$ to $u(x,t)$, we propose a more structured approach: using a CNN to capture spatial features and an LSTM for temporal evolution. This design leverages the structured grid information and the inherent temporal sequence nature of beam dynamics.

\section{Background and Model Formulation}
\subsection{Euler-Bernoulli Beam Equation}
The Euler-Bernoulli beam equation in its dynamic form is:
\begin{equation}
EI \frac{\partial^4 w}{\partial x^4}(x,t) + \mu \frac{\partial^2 w}{\partial t^2}(x,t) = q(x,t),
\end{equation}
where $w(x,t)$ is the beam deflection, $E$ is the Young's modulus, $I$ is the second moment of area, and $\mu$ is the linear density of the beam. The external load per unit length is given by $q(x,t)$. Appropriate boundary and initial conditions must be specified to solve this PDE \cite{timoshenko1959theory}.

\subsection{Physics-Informed Neural Networks (PINNs)}
PINNs enforce PDEs by embedding their residuals in the loss function. Let $u_\theta(x,t)$ denote the neural-network-based surrogate solution parameterized by weights $\theta$. The key idea is to compute PDE derivatives via automatic differentiation \cite{raissi2019physics} or finite differences and penalize the residual in a collocation-based manner.

\section{CNN-LSTM PINN Architecture}

\subsection{Spatial Feature Extraction with CNN}
For a structured spatial grid, we use a 1D CNN to learn local curvature and bending features of the beam deflection profile. Convolutional layers exploit translation-invariance along the beam length, capturing pertinent features for higher-order derivatives. The CNN compresses the input deflection field into a latent feature vector.

\subsection{Temporal Modeling with LSTM}
To handle time evolution, we feed the CNN-extracted feature vectors into an LSTM \cite{hochreiter1997long} that maintains a hidden state across time steps. The LSTM effectively captures the memory of past deflections, enabling learning of oscillatory or forced vibrations.

\subsection{Decoder and Unrolling}
An additional decoder maps the LSTM output back to the spatial field, producing a predicted deflection. By unrolling the network over $n$ time steps, the model sequentially generates the solution $u_\theta(x,t)$, thus learning the spatio-temporal dynamics in a data- and physics-driven manner.

\section{Loss Function and Training Procedure}
We define a multi-term loss that penalizes:
\begin{enumerate}
    \item PDE Residual: $\rho A \, u_{tt} + E I \, u_{xxxx} - q(x,t)$,
    \item Initial Conditions: $u(x,0) - w_0(x)$ and $u_t(x,0) - v_0(x)$,
    \item Boundary Conditions: $u(0,t) - b_0(t)$, etc.
\end{enumerate}
A total loss combines these via a weighted sum:
\begin{equation}
L = \lambda_{\text{PDE}} L_{\text{PDE}} + \lambda_{\text{IC}} L_{\text{IC}} + \lambda_{\text{BC}} L_{\text{BC}}.
\end{equation}
We iteratively backpropagate and update $\theta$ using an optimizer such as Adam \cite{kingma2015adam}.

\section{Implementation in PyTorch}
The proposed hybrid network is implemented in PyTorch \cite{paszke2019pytorch}. We first demonstrate the architectural details, followed by the PDE residual computations using finite differences or autograd.

\section{Attention Mechanisms}
To refine both spatial and temporal features, attention modules can be integrated:
\begin{itemize}
    \item \textbf{Spatial Attention:} Weighted masks on the CNN feature maps emphasize critical beam regions \cite{woo2018cbam}.
    \item \textbf{Temporal Attention:} An attention layer over past LSTM hidden states provides a direct mechanism to focus on significant prior time steps \cite{bahdanau2015neural}.
\end{itemize}
These attention strategies can improve both accuracy and interpretability.

\section{Experimental Results and Evaluation}
We validate the model by:
\begin{enumerate}
    \item Checking PDE residuals are near zero at collocation points,
    \item Comparing with analytical and finite-element solutions for known beam configurations,
    \item Verifying boundary and initial condition satisfaction,
    \item Assessing interpretability via attention visualizations.
\end{enumerate}

\section{Conclusion}
We have presented a novel hybrid CNN-LSTM PINN for solving the Euler-Bernoulli beam equation on structured grids, with the potential for further refinement through attention mechanisms. This framework effectively leverages both spatial locality and temporal memory, while adhering to the governing physics. Ongoing work includes extending this approach to more complex beam models and higher-dimensional PDEs.


%\subsection{Acknowledgments}

%Acknowledgments are encouraged as a way to thank those who have contributed to the research or project but did not merit being listed as an author. The Acknowledgments should indicate what each person did to contribute to the project.

%Authors can include an Acknowledgments section to recognize any advisory or financial help received. This section should appear after the Conclusions and before the references. Authors are responsible for ensuring that funding declarations match what was provided in the manuscript submission system as part of the FundRef query. Discrepancies may result in delays in publication.

% \label{section:references}
\bibliography{ascexmpl-new}
%

\appendix

\section{Appendix: Python Codes}
	\definecolor{codegreen}{rgb}{0,0.6,0}
	\definecolor{codegray}{rgb}{0.5,0.5,0.5}
	\definecolor{codepurple}{rgb}{0.58,0,0.82}
	%\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

	\lstdefinestyle{mystyle}{
		backgroundcolor=\color{white},   
		commentstyle=\color{codegreen},
		keywordstyle=\color{magenta},
		numberstyle=\tiny\color{codegray},
		stringstyle=\color{codepurple},
		basicstyle=\footnotesize,
		breakatwhitespace=false,         
		breaklines=true,                 
		captionpos=b,                    
		keepspaces=true,                 
		numbers=left,                    
		numbersep=5pt,                  
		showspaces=false,                
		showstringspaces=false,
		showtabs=false,                  
		tabsize=2
	}


    %\subsection{}
	\lstset{style=mystyle}
	\lstinputlisting[language=python]{code/v1.py}

\end{document}
